[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project 3/MLE.html",
    "href": "projects/project 3/MLE.html",
    "title": "BluePrinty Case Study",
    "section": "",
    "text": "import pandas as pd\n\ndf_bp = pd.read_csv('blueprinty.csv')\ndf_bp\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 4 columns\n\n\n\n\nunique_rows = df_bp.copy()\nunique_rows = unique_rows.drop_duplicates()\nunique_rows.shape\n\n(1145, 4)\n\n\n\nnum_duplicates = df_bp.shape[0] - unique_rows.shape[0]\nnum_duplicates\n\n355\n\n\n\nprint(df_bp.dtypes)\n\npatents         int64\nregion         object\nage           float64\niscustomer      int64\ndtype: object\n\n\n\ndf_bp.describe()\n\n\n\n\n\n\n\n\npatents\nage\niscustomer\n\n\n\n\ncount\n1500.000000\n1500.000000\n1500.000000\n\n\nmean\n3.684667\n26.357667\n0.320667\n\n\nstd\n2.352500\n7.242528\n0.466889\n\n\nmin\n0.000000\n9.000000\n0.000000\n\n\n25%\n2.000000\n21.000000\n0.000000\n\n\n50%\n3.000000\n26.000000\n0.000000\n\n\n75%\n5.000000\n31.625000\n1.000000\n\n\nmax\n16.000000\n49.000000\n1.000000\n\n\n\n\n\n\n\nCompare histograms and means of number of patents by customer status\n\n#Compare histograms and means of number of patents by customer status\n\nimport matplotlib.pyplot as plt\n\n# Clean column names if needed\ndf_bp.columns = df_bp.columns.str.strip().str.lower().str.replace('#', '').str.replace(' ', '_')\n\n# Separate data\ncustomers = df_bp[df_bp['iscustomer'] == 1]\nnon_customers = df_bp[df_bp['iscustomer'] == 0]\n\n# Plot histograms\nplt.figure(figsize=(10, 5))\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers')\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers')\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.legend()\nplt.show()\n\n# Compare means\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f\"Mean number of patents (Customers): {mean_customers:.2f}\")\nprint(f\"Mean number of patents (Non-Customers): {mean_non_customers:.2f}\")\n\n\n\n\n\n\n\n\n\n\nMean number of patents (Customers): 4.13\nMean number of patents (Non-Customers): 3.47\n\n\n\ntotal_customers = df_bp[df_bp['iscustomer'] == 1].shape[0]\ntotal_non_customers = df_bp[df_bp['iscustomer'] == 0].shape[0]\n\nprint(f\"Total Customers: {total_customers}\")\nprint(f\"Total Non-Customers: {total_non_customers}\")\n\nTotal Customers: 481\nTotal Non-Customers: 1019"
  },
  {
    "objectID": "projects/project 3/MLE.html#airbnb",
    "href": "projects/project 3/MLE.html#airbnb",
    "title": "BluePrinty Case Study",
    "section": "Airbnb",
    "text": "Airbnb\n\nimport pandas as pd\ndf_ab = pd.read_csv('airbnb.csv')\n\n\ndf_ab.shape\n\n(40628, 14)\n\n\n\ndf_ab.columns\n\nIndex(['Unnamed: 0', 'id', 'days', 'last_scraped', 'host_since', 'room_type',\n       'bathrooms', 'bedrooms', 'price', 'number_of_reviews',\n       'review_scores_cleanliness', 'review_scores_location',\n       'review_scores_value', 'instant_bookable'],\n      dtype='object')\n\n\n\n#Drop missing values \ncols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf = df_ab[cols].dropna()\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Histograms for numerical features\nnumeric_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms',\n                'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\ndf[numeric_cols].hist(bins=30, figsize=(14, 10))\nplt.tight_layout()\nplt.show()\n\n# Boxplot of number_of_reviews by room_type\nsns.boxplot(x='room_type', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Room Type')\nplt.show()\n\n# Boxplot of number_of_reviews by instant_bookable\nsns.boxplot(x='instant_bookable', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Instant Bookable Status')\nplt.show()\n\n#Boxplot of number_of_reviews by bedrooms\nsns.boxplot(x='bedrooms', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Bedrooms')\nplt.show()\n\n# Correlation heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Plot distribution of number_of_reviews\nplt.figure(figsize=(8, 5))\nsns.histplot(df['number_of_reviews'], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Average number of reviews by room_type\nplt.figure(figsize=(8, 5))\nsns.barplot(x='room_type', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Room Type\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n# Average number of reviews by instant_bookable\nplt.figure(figsize=(8, 5))\nsns.barplot(x='instant_bookable', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Instant Bookable Status\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n# Average number of reviews by number of bedrooms\nplt.figure(figsize=(8, 5))\nsns.barplot(x='bedrooms', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Number of Bedrooms\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n# Average number of reviews by room_type and instant_bookable (interaction)\nplt.figure(figsize=(10, 6))\nsns.barplot(x='room_type', y='number_of_reviews', hue='instant_bookable', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Room Type and Instant Bookability\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.legend(title='Instant Bookable')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Create dummy variables for room type\nroom_dummies = pd.get_dummies(df[\"room_type\"], drop_first=True)\n\n# Convert 'instant_bookable' to binary\ndf[\"instant_bookable\"] = df[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Construct the design matrix\nX = pd.concat([\n    df[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n        \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n        \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\n# Add intercept and ensure all values are float\nX = sm.add_constant(X)\nX = X.astype(float)\n\n# Define the target variable\nY = df[\"number_of_reviews\"]\n\n# Fit Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Format output\nsummary_df = poisson_results.summary2().tables[1]\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P&gt;|z|\": \"P-Value\"\n})\n\n# Filter and round significant results\nsignificant_results = summary_df[summary_df[\"P-Value\"] &lt; 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\nsignificant_results = significant_results.round(4)\n\nprint(significant_results)\n\n                           Coefficient  Std. Error  P-Value\nconst                           3.4980      0.0161   0.0000\nprice                          -0.0000      0.0000   0.0315\ndays                            0.0001      0.0000   0.0000\nbathrooms                      -0.1177      0.0037   0.0000\nbedrooms                        0.0741      0.0020   0.0000\nreview_scores_cleanliness       0.1131      0.0015   0.0000\nreview_scores_location         -0.0769      0.0016   0.0000\nreview_scores_value            -0.0911      0.0018   0.0000\ninstant_bookable                0.3459      0.0029   0.0000\nPrivate room                   -0.0105      0.0027   0.0001\nShared room                    -0.2463      0.0086   0.0000"
  },
  {
    "objectID": "projects/project 2/karlanlist.html",
    "href": "projects/project 2/karlanlist.html",
    "title": "Nitya's Website and Portfolio",
    "section": "",
    "text": "import pandas as pd\n\nfile_path = 'karlan_list_2007.dta'\ndf1 = pd.read_stata(file_path)\n\nprint(df1.head())\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\nEDA and Descriptive Analysis\ndf1.columns\n\nIndex(['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25',\n       'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1',\n       'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra',\n       'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple',\n       'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse',\n       'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush',\n       'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack',\n       'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba',\n       'pop_propurban'],\n      dtype='object')\ndf1.dtypes\n\ntreatment                 int8\ncontrol                   int8\nratio                 category\nratio2                    int8\nratio3                    int8\nsize                  category\nsize25                    int8\nsize50                    int8\nsize100                   int8\nsizeno                    int8\nask                   category\naskd1                     int8\naskd2                     int8\naskd3                     int8\nask1                     int16\nask2                     int16\nask3                     int16\namount                 float32\ngave                      int8\namountchange           float32\nhpa                    float32\nltmedmra                  int8\nfreq                     int16\nyears                  float64\nyear5                     int8\nmrm2                   float64\ndormant                   int8\nfemale                 float64\ncouple                 float64\nstate50one                int8\nnonlit                 float64\ncases                  float64\nstatecnt               float32\nstateresponse          float32\nstateresponset         float32\nstateresponsec         float32\nstateresponsetminc     float32\nperbush                float32\nclose25                float64\nred0                   float64\nblue0                  float64\nredcty                 float64\nbluecty                float64\npwhite                 float32\npblack                 float32\npage18_39              float32\nave_hh_sz              float32\nmedian_hhincome        float64\npowner                 float32\npsch_atlstba           float32\npop_propurban          float32\ndtype: object\ncolumn_means = df1.mean(numeric_only=True)\nprint(\"Means of each column:\")\nprint(column_means)\n\nMeans of each column:\ntreatment                 0.666813\ncontrol                   0.333187\nratio2                    0.222311\nratio3                    0.222211\nsize25                    0.166723\nsize50                    0.166623\nsize100                   0.166723\nsizeno                    0.166743\naskd1                     0.222311\naskd2                     0.222291\naskd3                     0.222211\nask1                     71.501807\nask2                     91.792724\nask3                    111.046263\namount                    0.915694\ngave                      0.020646\namountchange            -52.672016\nhpa                      59.384975\nltmedmra                  0.493720\nfreq                      8.039355\nyears                     6.097540\nyear5                     0.508815\nmrm2                     13.007268\ndormant                   0.523471\nfemale                    0.277669\ncouple                    0.091897\nstate50one                0.000998\nnonlit                    2.473918\ncases                     1.499768\nstatecnt                  5.998820\nstateresponse             0.020627\nstateresponset            0.021989\nstateresponsec            0.017717\nstateresponsetminc        0.004273\nperbush                   0.487940\nclose25                   0.185702\nred0                      0.404452\nblue0                     0.595548\nredcty                    0.510245\nbluecty                   0.488715\npwhite                    0.819599\npblack                    0.086710\npage18_39                 0.321694\nave_hh_sz                 2.429012\nmedian_hhincome       54815.700533\npowner                    0.669418\npsch_atlstba              0.391661\npop_propurban             0.871968\ndtype: float64\n# Check for missing values\nmissing_values = df1.isnull().sum()\nprint(\"Missing values in each column:\")\nprint(missing_values)\n\nMissing values in each column:\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\ndf1['gender'] = df1['female'].apply(lambda x: 'F' if x == 1 else 'M')\ndf = df1.copy()\n\ncategorical_vars = [\n    'ratio', 'ratio2', 'ratio3',\n    'size', 'size25', 'size50', 'size100', 'sizeno',\n    'askd1', 'askd2', 'askd3',\n    'ltmedmra', 'year5', 'dormant',\n    'female', 'couple', 'state50one', \n    'close25', 'red0', 'blue0', 'redcty', 'bluecty'\n]\n\n# Convert to categorical\ndf[categorical_vars] = df[categorical_vars].astype('category')\n\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nask1\nask2\nask3\namount\ngave\namountchange\nhpa\nfreq\n...\nstateresponsetminc\nperbush\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n50080.000000\n50048.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n71.501807\n91.792724\n111.046263\n0.915694\n0.020646\n-52.672016\n59.384975\n8.039355\n...\n0.004273\n0.487940\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n101.728936\n127.252628\n151.673562\n8.707393\n0.142197\n1267.097656\n71.179871\n11.394454\n...\n0.009112\n0.078733\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n25.000000\n35.000000\n50.000000\n0.000000\n0.000000\n-200412.125000\n0.000000\n0.000000\n...\n-0.047619\n0.090909\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n35.000000\n45.000000\n55.000000\n0.000000\n0.000000\n-50.000000\n30.000000\n2.000000\n...\n-0.001388\n0.444444\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n45.000000\n60.000000\n70.000000\n0.000000\n0.000000\n-30.000000\n45.000000\n4.000000\n...\n0.001779\n0.484848\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n65.000000\n85.000000\n100.000000\n0.000000\n0.000000\n-25.000000\n60.000000\n10.000000\n...\n0.010545\n0.525253\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1500.000000\n1875.000000\n2250.000000\n400.000000\n1.000000\n275.000000\n1000.000000\n218.000000\n...\n0.111111\n0.731959\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 28 columns\ndatadesc = df.describe()\n%pip install tabulate\n\nprint(datadesc.to_markdown(index=False))\n\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.12/site-packages (0.9.0)\nNote: you may need to restart the kernel to use updated packages.\n|    treatment |      control |       ask1 |       ask2 |      ask3 |       amount |          gave |   amountchange |        hpa |        freq |       years |       mrm2 |      nonlit |       cases |       statecnt |   stateresponse |   stateresponset |   stateresponsec |   stateresponsetminc |       perbush |         pwhite |        pblack |    page18_39 |    ave_hh_sz |   median_hhincome |       powner |   psch_atlstba |   pop_propurban |\n|-------------:|-------------:|-----------:|-----------:|----------:|-------------:|--------------:|---------------:|-----------:|------------:|------------:|-----------:|------------:|------------:|---------------:|----------------:|-----------------:|-----------------:|---------------------:|--------------:|---------------:|--------------:|-------------:|-------------:|------------------:|-------------:|---------------:|----------------:|\n| 50083        | 50083        | 50083      | 50083      | 50083     | 50083        | 50083         |      50083     | 50083      | 50083       | 50082       | 50082      | 49631       | 49631       | 50083          |   50083         |   50083          |   50080          |       50080          | 50048         | 48217          | 48047         | 48217        | 48221        |           48209   | 48214        |   48215        |    48217        |\n|     0.666813 |     0.333187 |    71.5018 |    91.7927 |   111.046 |     0.915694 |     0.0206457 |        -52.672 |    59.385  |     8.03935 |     6.09754 |    13.0073 |     2.47392 |     1.49977 |     5.99882    |       0.0206269 |       0.0219885  |       0.0177167  |           0.00427312 |     0.48794   |     0.819599   |     0.0867098 |     0.321694 |     2.42901  |           54815.7 |     0.669418 |       0.391661 |        0.871968 |\n|     0.471357 |     0.471357 |   101.729  |   127.253  |   151.674 |     8.70739  |     0.142197  |       1267.1   |    71.1799 |    11.3945  |     5.50349 |    12.0814 |     1.96153 |     1.15514 |     5.74599    |       0.0051708 |       0.00625721 |       0.00751621 |           0.00911209 |     0.0787327 |     0.16856    |     0.135868  |     0.103039 |     0.378105 |           22027.3 |     0.193405 |       0.186599 |        0.258633 |\n|     0        |     0        |    25      |    35      |    50     |     0        |     0         |    -200412     |     0      |     0       |     0       |     0      |     0       |     0       |     0.00199481 |       0         |       0          |       0          |          -0.047619   |     0.0909091 |     0.00941798 |     0         |     0        |     0        |            5000   |     0        |       0        |        0        |\n|     0        |     0        |    35      |    45      |    55     |     0        |     0         |        -50     |    30      |     2       |     2       |     4      |     1       |     1       |     1.83323    |       0.0181635 |       0.0184932  |       0.0128617  |          -0.00138826 |     0.444444  |     0.755845   |     0.0147292 |     0.258311 |     2.21     |           39181   |     0.560222 |       0.235647 |        0.884929 |\n|     1        |     0        |    45      |    60      |    70     |     0        |     0         |        -30     |    45      |     4       |     5       |     8      |     3       |     1       |     3.5388     |       0.0197095 |       0.0216972  |       0.0198814  |           0.00177869 |     0.484848  |     0.872797   |     0.0365541 |     0.305534 |     2.44     |           50673   |     0.712296 |       0.373744 |        1        |\n|     1        |     1        |    65      |    85      |   100     |     0        |     0         |        -25     |    60      |    10       |     9       |    19      |     4       |     2       |     9.60702    |       0.0230482 |       0.0247027  |       0.0208062  |           0.0105448  |     0.525253  |     0.938827   |     0.090882  |     0.369132 |     2.66     |           66005   |     0.816798 |       0.530036 |        1        |\n|     1        |     1        |  1500      |  1875      |  2250     |   400        |     1         |        275     |  1000      |   218       |    95       |   168      |     6       |     4       |    17.3688     |       0.0769231 |       0.111111   |       0.0526316  |           0.111111   |     0.731959  |     1          |     0.989622  |     0.997544 |     5.27     |          200001   |     1        |       1        |        1        |\n# Generate markdown table to insert into Quarto document\nmarkdown_table = datadesc.to_markdown(index=False)\n\nwith open(\"index.qmd\", \"r\") as f:\n    qmd_text = f.read()\n\nqmd_text = qmd_text.replace(\"&lt;!-- INSERT_DESC_HERE --&gt;\", markdown_table)\n\nwith open(\"index.qmd\", \"w\") as f:\n    f.write(qmd_text)\n# Replace 'give' with the correct column name, e.g., 'gave', which exists in the dfframe\ndfg = df[df['gave'] == 1]\npercentage_gave = (df['gave'].astype(int).sum() / len(df)) * 100\nprint(f\"Percentage of rows where gave = 1: {percentage_gave:.2f}%\")\n\nPercentage of rows where gave = 1: 2.06%\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Key variables and experimental treatment variables to explore\nvariables_to_plot = ['hpa', 'freq', 'years', 'mrm2', 'median_hhincome', \n                     'ratio', 'size', 'ask', 'askd1', 'askd2', 'askd3', ]\n\nimport math\n\n\n\n# Set up the figure layout\nnum_vars = len(variables_to_plot)\nnum_cols = 3\nnum_rows = math.ceil(num_vars / num_cols)\n\nplt.figure(figsize=(18, 5 * num_rows))\n\nfor idx, var in enumerate(variables_to_plot, 1):\n    plt.subplot(num_rows, num_cols, idx)\n    if df[var].dtype in ['int64', 'float64', 'float32', 'int16', 'int8']:\n        sns.histplot(data=df, x=var, hue='gave', multiple='dodge', bins=30)\n        plt.title(f'Histogram of {var} by Gave')\n    else:\n        sns.countplot(data=df, x=var, hue='gave')\n        plt.title(f'Count of {var} by Gave')\n    plt.xlabel(var)\n    plt.ylabel('Count')\n    plt.tight_layout()\n\nplt.show()\n# Key variables and experimental treatment variables to explore\nvariables_to_plot = ['hpa', 'freq', 'years', 'mrm2', 'median_hhincome', \n                     'ratio', 'size', 'ask']\n\n# Set up the figure layout\nnum_vars = len(variables_to_plot)\nnum_cols = 3\nnum_rows = -(-num_vars // num_cols)  # Ceiling division\n\nplt.figure(figsize=(18, 5 * num_rows))\n\n# Plot distribution of donation amount against each variable\nfor idx, var in enumerate(variables_to_plot, 1):\n    plt.subplot(num_rows, num_cols, idx)\n    if var in ['freq', 'hpa']:\n        sns.histplot(data=dfg, x=var, bins=10, kde=True)\n        plt.title(f'Binned Histogram of {var}')\n    elif dfg[var].dtype in ['int64', 'float64']:\n        sns.histplot(data=dfg, x=var, bins=30, kde=True)\n        plt.title(f'Distribution of {var}')\n    else:\n        sns.countplot(data=dfg, x=var)\n        plt.title(f'Frequency of {var}')\n    plt.tight_layout()\n\nplt.show()\ndfg[['ltmedmra', 'year5', 'dormant','red0']] = dfg[['ltmedmra', 'year5', 'dormant','red0']].astype('category')\n\n/tmp/ipykernel_5207/3543977266.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dfg[['ltmedmra', 'year5', 'dormant','red0']] = dfg[['ltmedmra', 'year5', 'dormant','red0']].astype('category')\ndonor_characteristics = ['ltmedmra', 'year5', 'gender']\ngeo_political_context = ['red0', 'perbush', 'nonlit']\nzip_code_demographics = ['median_hhincome', 'powner', 'psch_atlstba']\n\n# Combine into one list if needed\nselected_vars = donor_characteristics + geo_political_context + zip_code_demographics\n\nnum_vars = len(variables_to_plot)\nnum_cols = 3\nnum_rows = -(-num_vars // num_cols)  # Ceiling division\n\nplt.figure(figsize=(18, 5 * num_rows))\n\n# Plot distribution or frequency plots for selected_vars against 'amount'\nfor idx, var in enumerate(selected_vars, 1):\n    plt.subplot(num_rows, num_cols, idx)\n    if df[var].dtype in ['int64', 'float64', 'float32', 'int16', 'int8']:\n        sns.histplot(data=dfg, x=var, bins=10, kde=True)\n        plt.title(f' Plot of {var} vs Amount')\n    else:\n        sns.countplot(data=dfg, x=var)\n        plt.title(f'Frequency of {var}')\n    plt.xlabel(var)\n    plt.ylabel('Amount')\n    plt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "projects/project 2/karlanlist.html#hypothesis-tests-to-confirm-if-non-outcome-variables-are-not-significantly-different-in-control-and-treatment-groups",
    "href": "projects/project 2/karlanlist.html#hypothesis-tests-to-confirm-if-non-outcome-variables-are-not-significantly-different-in-control-and-treatment-groups",
    "title": "Nitya's Website and Portfolio",
    "section": "Hypothesis tests to confirm if non-outcome variables are not significantly different in control and treatment groups",
    "text": "Hypothesis tests to confirm if non-outcome variables are not significantly different in control and treatment groups\nat 95% confidence level\n\ndf.columns\n\nIndex(['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25',\n       'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1',\n       'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra',\n       'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple',\n       'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse',\n       'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush',\n       'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack',\n       'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba',\n       'pop_propurban', 'gender'],\n      dtype='object')\n\n\nMRM2: Months since last donation\n\ntreatment_group_mrm2 = df[df['treatment'] == 1]['mrm2'].dropna()\ncontrol_group_mrm2 = df[df['control'] == 1]['mrm2'].dropna()\n\n# Calculate t-statistic\nmean_diff = treatment_group_mrm2.mean() - control_group_mrm2.mean()\nvar_treatment = treatment_group_mrm2.var(ddof=1)\nvar_control = control_group_mrm2.var(ddof=1)\nn_treatment = len(treatment_group_mrm2)\nn_control = len(control_group_mrm2)\n\npooled_se = ((var_treatment / n_treatment) + (var_control / n_control)) ** 0.5\nmanual_t_stat = mean_diff / pooled_se\n\nprint(f\"Manual T-statistic: {manual_t_stat:.4f}\")\n\nfrom scipy.stats import t\n\n# Degrees of freedom\ndf_degrees = n_treatment + n_control - 2\n\n# Critical value for 95% confidence level (two-tailed test)\ncritical_value = t.ppf(1 - 0.025, df_degrees)\n\nprint(f\"Critical value (95% confidence level): {critical_value:.4f}\")\n\n# Compare manual_t_stat against critical value\nif abs(manual_t_stat) &gt; critical_value:\n    print(\"Reject the null hypothesis: There is a statistically significant difference in mrm2 between treatment and control groups.\")\nelse:\n    print(\"Fail to reject the null hypothesis: There is no statistically significant difference in mrm2 between treatment and control groups.\")\n\nManual T-statistic: 0.1195\nCritical value (95% confidence level): 1.9600\nFail to reject the null hypothesis: There is no statistically significant difference in mrm2 between treatment and control groups.\n\n\n\n#crosss check with scipy stats\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import t\n\nt_stat, p_value = ttest_ind(treatment_group_mrm2, control_group_mrm2, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\n# Check significance at 95% confidence level\nif p_value &lt; 0.05:\n    print(\"The difference in mrm2 between treatment and control groups is statistically significant.\")\nelse:\n    print(\"The difference in mrm2 between treatment and control groups is not statistically significant.\")\n\nT-statistic: 0.1195\nP-value: 0.9049\nThe difference in mrm2 between treatment and control groups is not statistically significant.\n\n\nConfirm with linear regression:\nModel: mrm2 = β0 + β1 * treatment + ε\nThe coefficient β1 tells you the mean difference between treatment and control. The p-value on β1 tells you if this difference is statistically significant. Epsilon is the error term, which captures all other factors that might affect mrm2.\n\n#using linear regression to confirm\n\n\nimport statsmodels.api as sm\n\nX = df['treatment']  # Independent variable\ny = df['mrm2']       # Dependent variable\n\nX = sm.add_constant(X)\n\nmodel = sm.OLS(y, X, missing='drop').fit()\nprint(model.summary())\n\nprint(f\"T-value for treatment: {model.tvalues['treatment']:.4f}\")\nprint(f\"P-value for treatment: {model.pvalues['treatment']:.4f}\")\n\np_valuem = model.pvalues['treatment']\nt_valuem = model.tvalues['treatment']\n\n# Check the p-value of the treatment coefficient\nif model.pvalues['treatment'] &lt; 0.05:\n    print(\"The treatment effect on mrm2 is statistically significant.\")\nelse:\n    print(\"The treatment effect on mrm2 is not statistically significant.\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):              0.905\nTime:                        19:24:38   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-value for treatment: 0.1195\nP-value for treatment: 0.9049\nThe treatment effect on mrm2 is not statistically significant.\n\n\nPerform hypothesis tests on other variable, picking few from each “bucket” of variables.\ndonor_characteristics = [‘ltmedmra’, ‘year5’, ‘gender’]\ngeo_political_context = [‘red0’, ‘perbush’, ‘nonlit’]\nzip_code_demographics = [‘median_hhincome’, ‘powner’, ‘psch_atlstba’]\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import t\n\n\ndonor_characteristics = ['freq', 'year5', 'female','mrm2']  \ngeo_political_context = ['red0', 'perbush', 'nonlit']\nzip_code_demographics = ['median_hhincome', 'powner', 'psch_atlstba']\n\nall_vars = donor_characteristics + geo_political_context + zip_code_demographics\n\n# Set significance level\nalpha = 0.05\n\n# Store results\nresults = []\n\n\n\nfor v in all_vars:\n    # Drop missing data\n    treatment = df[df['treatment'] == 1][v].dropna()\n    control = df[df['control'] == 1][v].dropna()\n\n    # Convert categorical variables to numeric if necessary\n    if treatment.dtype.name == 'category':\n        treatment = treatment.astype(int)\n    if control.dtype.name == 'category':\n        control = control.astype(int)\n\n    # Manual t-test\n    mean_diff = treatment.mean() - control.mean()\n    var_treatment = treatment.var(ddof=1)\n    var_control = control.var(ddof=1)\n    n_treatment = len(treatment)\n    n_control = len(control)\n\n    pooled_se = ((var_treatment / n_treatment) + (var_control / n_control))**0.5\n    manual_t_stat = mean_diff / pooled_se\n    df_degrees = n_treatment + n_control - 2\n    critical_value = t.ppf(1 - alpha/2, df_degrees)\n    manual_conclusion = \"Significant\" if abs(manual_t_stat) &gt; critical_value else \"Not Significant\"\n    \n    # Regression\n    X = df['treatment']\n    y = df[v]\n    X = sm.add_constant(X)\n\n    model = sm.OLS(y, X, missing='drop').fit(cov_type='HC1')\n\n\n    reg_t_stat = round(model.tvalues['treatment'], 4)\n    reg_p_value = round(model.pvalues['treatment'], 4)\n    reg_conclusion = \"Significant\" if reg_p_value &lt; alpha else \"Not Significant\"\n    \n\n    # Store the results\n    results.append({\n        \"Variable\": v,\n        \"Manual T-Stat\": round(manual_t_stat, 4),\n        \"Critical Value\": round(critical_value, 4),\n        \"Manual Test Conclusion\": manual_conclusion,\n        \"Regression T-Stat\": reg_t_stat,\n        \"Regression P-Value\": reg_p_value,\n        \"Regression Conclusion\": reg_conclusion\n    })\n\nresults_df = pd.DataFrame(results)\n\nresults_df\n\n\n\n\n\n\n\n\nVariable\nManual T-Stat\nCritical Value\nManual Test Conclusion\nRegression T-Stat\nRegression P-Value\nRegression Conclusion\n\n\n\n\n0\nfreq\n-0.1108\n1.96\nNot Significant\n-0.1108\n0.9117\nNot Significant\n\n\n1\nyear5\n-1.5627\n1.96\nNot Significant\n-1.5627\n0.1181\nNot Significant\n\n\n2\nfemale\n-1.7535\n1.96\nNot Significant\n-1.7535\n0.0795\nNot Significant\n\n\n3\nmrm2\n0.1195\n1.96\nNot Significant\n0.1195\n0.9049\nNot Significant\n\n\n4\nred0\n1.8773\n1.96\nNot Significant\n1.8773\n0.0605\nNot Significant\n\n\n5\nperbush\n2.7463\n1.96\nSignificant\n2.7463\n0.0060\nSignificant\n\n\n6\nnonlit\n1.7052\n1.96\nNot Significant\n1.7052\n0.0882\nNot Significant\n\n\n7\nmedian_hhincome\n-0.7433\n1.96\nNot Significant\n-0.7433\n0.4573\nNot Significant\n\n\n8\npowner\n0.1895\n1.96\nNot Significant\n0.1894\n0.8498\nNot Significant\n\n\n9\npsch_atlstba\n-1.8427\n1.96\nNot Significant\n-1.8427\n0.0654\nNot Significant\n\n\n\n\n\n\n\n\nprint(results_df.to_markdown(index=False))\n\n| Variable        |   Manual T-Stat |   Critical Value | Manual Test Conclusion   |   Regression T-Stat |   Regression P-Value | Regression Conclusion   |\n|:----------------|----------------:|-----------------:|:-------------------------|--------------------:|---------------------:|:------------------------|\n| freq            |         -0.1108 |             1.96 | Not Significant          |             -0.1108 |               0.9117 | Not Significant         |\n| year5           |         -1.5627 |             1.96 | Not Significant          |             -1.5627 |               0.1181 | Not Significant         |\n| female          |         -1.7535 |             1.96 | Not Significant          |             -1.7535 |               0.0795 | Not Significant         |\n| mrm2            |          0.1195 |             1.96 | Not Significant          |              0.1195 |               0.9049 | Not Significant         |\n| red0            |          1.8773 |             1.96 | Not Significant          |              1.8773 |               0.0605 | Not Significant         |\n| perbush         |          2.7463 |             1.96 | Significant              |              2.7463 |               0.006  | Significant             |\n| nonlit          |          1.7052 |             1.96 | Not Significant          |              1.7052 |               0.0882 | Not Significant         |\n| median_hhincome |         -0.7433 |             1.96 | Not Significant          |             -0.7433 |               0.4573 | Not Significant         |\n| powner          |          0.1895 |             1.96 | Not Significant          |              0.1894 |               0.8498 | Not Significant         |\n| psch_atlstba    |         -1.8427 |             1.96 | Not Significant          |             -1.8427 |               0.0654 | Not Significant         |\n\n\n\n# Generate markdown table\nmarkdown_table = results_df.to_markdown(index=False)\n\n# Read the original .qmd content\nwith open(\"index.qmd\", \"r\") as f:\n    qmd_text = f.read()\n\n# Replace the placeholder with the table\nqmd_text = qmd_text.replace(\"&lt;!-- INSERT_RESULTS_HERE --&gt;\", markdown_table)\n\n# Write back the updated file\nwith open(\"index.qmd\", \"w\") as f:\n    f.write(qmd_text)"
  },
  {
    "objectID": "projects/project 2/karlanlist.html#chairtable-contribution-made",
    "href": "projects/project 2/karlanlist.html#chairtable-contribution-made",
    "title": "Nitya's Website and Portfolio",
    "section": "Chairtable contribution made",
    "text": "Chairtable contribution made\n\n# Calculate the proportion of people who donated in each group\ntreatment_proportion = df[df['treatment'] == 1]['gave'].mean()\ncontrol_proportion = df[df['control'] == 1]['gave'].mean()\n\nplt.figure(figsize=(8, 6))\nax = sns.barplot(x=['Treatment', 'Control'], y=[treatment_proportion, control_proportion], palette='viridis')\n\nfor i, proportion in enumerate([treatment_proportion, control_proportion]):\n    plt.text(i, proportion + 0.001, f'{proportion:.3f}', ha='center', va='bottom', fontsize=10)\n\nplt.title('Proportion of People Who Donated by Group')\nplt.ylabel('Proportion')\nplt.xlabel('Group')\nplt.ylim(0, 0.05)\nplt.show()\n\n/tmp/ipykernel_5207/963862486.py:6: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  ax = sns.barplot(x=['Treatment', 'Control'], y=[treatment_proportion, control_proportion], palette='viridis')\n\n\n\n\n\n\n\n\n\n\n# Perform a bivariate linear regression\nX = df['treatment']  # Independent variable\ny = df['gave']       # Dependent variable (binary outcome)\n\nX = sm.add_constant(X)  # Add a constant for the regression\nmodel = sm.OLS(y, X, missing='drop').fit()\n\n# Print the regression summary\nprint(model.summary())\n\n# Extract the p-value and coefficient for interpretation\np_value = model.pvalues['treatment']\ncoef = model.params['treatment']\n\n# Interpretation\nif p_value &lt; 0.05:\n    print(f\"The treatment effect is statistically significant (p-value: {p_value:.4f}).\")\n    print(f\"The coefficient for treatment is {coef:.4f}, indicating that being in the treatment group increases the likelihood of making a charitable donation by {coef * 100:.2f} percentage points.\")\nelse:\n    print(f\"The treatment effect is not statistically significant (p-value: {p_value:.4f}).\")\n    print(\"There is no evidence to suggest that being in the treatment group affects the likelihood of making a charitable donation.\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        19:24:39   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nThe treatment effect is statistically significant (p-value: 0.0019).\nThe coefficient for treatment is 0.0042, indicating that being in the treatment group increases the likelihood of making a charitable donation by 0.42 percentage points.\n\n\n\nresults_bl = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.values,\n    \"Standard Error\": model.bse.values,\n    \"t-Statistic\": model.tvalues.values,\n    \"p-Value\": model.pvalues.values,\n    \"CI Lower (95%)\": model.conf_int()[0].values,\n    \"CI Upper (95%)\": model.conf_int()[1].values\n})\n\n\nprint(results_bl.to_markdown(index=False))\nmarkdown_table = results_bl.to_markdown(index=False)\n\n# Read the original .qmd content\nwith open(\"index.qmd\", \"r\") as f:\n    qmd_text = f.read()\n\n# Replace the placeholder with the table\nqmd_text = qmd_text.replace(\"&lt;!-- INSERT_RESULTSb1_HERE --&gt;\", markdown_table)\n\n# Write back the updated file\nwith open(\"index.qmd\", \"w\") as f:\n    f.write(qmd_text)\n\n| Variable   |   Coefficient |   Standard Error |   t-Statistic |     p-Value |   CI Lower (95%) |   CI Upper (95%) |\n|:-----------|--------------:|-----------------:|--------------:|------------:|-----------------:|-----------------:|\n| const      |    0.0178582  |       0.00110068 |      16.2246  | 4.77903e-59 |       0.0157009  |       0.0200156  |\n| treatment  |    0.00418035 |       0.00134791 |       3.10136 | 0.0019274   |       0.00153844 |       0.00682227 |\n\n\n\nProbit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control\n\nimport statsmodels.api as sm\n\nX = df['treatment']  # Independent variable\ny = df['gave']       # Dependent variable (binary outcome)\n\nX = sm.add_constant(X)\n\nprobit_model = sm.Probit(y, X, missing='drop').fit()\n\nprint(probit_model.summary())\n\np_value_probit = probit_model.pvalues['treatment']\ncoef_probit = probit_model.params['treatment']\n\n# Interpretation\nif p_value_probit &lt; 0.05:\n    print(f\"The treatment effect is statistically significant (p-value: {p_value_probit:.4f}).\")\n    print(f\"The coefficient for treatment is {coef_probit:.4f}, indicating that being in the treatment group has a significant effect on the likelihood of making a charitable donation.\")\nelse:\n    print(f\"The treatment effect is not statistically significant (p-value: {p_value_probit:.4f}).\")\n    print(\"There is no evidence to suggest that being in the treatment group affects the likelihood of making a charitable donation.\")\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 21 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        19:24:40   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\nThe treatment effect is statistically significant (p-value: 0.0019).\nThe coefficient for treatment is 0.0868, indicating that being in the treatment group has a significant effect on the likelihood of making a charitable donation.\n\n\n\nresults_pr = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.values,\n    \"Standard Error\": model.bse.values,\n    \"t-Statistic\": model.tvalues.values,\n    \"p-Value\": model.pvalues.values,\n    \"CI Lower (95%)\": model.conf_int()[0].values,\n    \"CI Upper (95%)\": model.conf_int()[1].values\n})\n\n\nprint(results_pr.to_markdown(index=False))\nmarkdown_table = results_pr.to_markdown(index=False)\n\n# Read the original .qmd content\nwith open(\"index.qmd\", \"r\") as f:\n    qmd_text = f.read()\n\n# Replace the placeholder with the table\nqmd_text = qmd_text.replace(\"&lt;!-- INSERT_RESULTSpr_HERE --&gt;\", markdown_table)\n\n# Write back the updated file\nwith open(\"index.qmd\", \"w\") as f:\n    f.write(qmd_text)\n\n| Variable   |   Coefficient |   Standard Error |   t-Statistic |     p-Value |   CI Lower (95%) |   CI Upper (95%) |\n|:-----------|--------------:|-----------------:|--------------:|------------:|-----------------:|-----------------:|\n| const      |    0.0178582  |       0.00110068 |      16.2246  | 4.77903e-59 |       0.0157009  |       0.0200156  |\n| treatment  |    0.00418035 |       0.00134791 |       3.10136 | 0.0019274   |       0.00153844 |       0.00682227 |\n\n\n\n\nDifferences between Match Rates\n\nprint(\"Unique values in 'ratio':\", df['ratio'].unique())\nprint(\"Unique values in 'ratio2':\", df['ratio2'].unique())\nprint(\"Unique values in 'ratio3':\", df['ratio3'].unique())\n\nUnique values in 'ratio': ['Control', 1, 2, 3]\nCategories (4, object): ['Control' &lt; 1 &lt; 2 &lt; 3]\nUnique values in 'ratio2': [0, 1]\nCategories (2, int8): [0, 1]\nUnique values in 'ratio3': [0, 1]\nCategories (2, int8): [0, 1]\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Create group masks\nis_1to1 = (df['treatment'] == 1) & (df['ratio2'] == 0) & (df['ratio3'] == 0)\nis_2to1 = df['ratio2'] == 1\nis_3to1 = df['ratio3'] == 1\n\n# Get gave outcomes for each group\ngave_1to1 = df[is_1to1]['gave'].dropna()\ngave_2to1 = df[is_2to1]['gave'].dropna()\ngave_3to1 = df[is_3to1]['gave'].dropna()\n\n# Run t-tests\nt_stat_21, p_val_21 = ttest_ind(gave_2to1, gave_1to1, equal_var=False)\nt_stat_31, p_val_31 = ttest_ind(gave_3to1, gave_1to1, equal_var=False)\nt_stat_32, p_val_32 = ttest_ind(gave_3to1, gave_2to1, equal_var=False)\n\n# Print results\nprint(f\"2:1 vs 1:1 → t = {t_stat_21:.4f}, p = {p_val_21:.4f}\")\nprint(f\"3:1 vs 1:1 → t = {t_stat_31:.4f}, p = {p_val_31:.4f}\")\nprint(f\"3:1 vs 2:1 → t = {t_stat_32:.4f}, p = {p_val_32:.4f}\")\n\n2:1 vs 1:1 → t = 0.9650, p = 0.3345\n3:1 vs 1:1 → t = 1.0150, p = 0.3101\n3:1 vs 2:1 → t = 0.0501, p = 0.9600\n\n\n\nfrom scipy.stats import t\n\nalpha = 0.05  # for 95% confidence\nresults = []\n\n# Group definitions\nis_1to1 = (df['treatment'] == 1) & (df['ratio2'] == 0) & (df['ratio3'] == 0)\nis_2to1 = df['ratio2'] == 1\nis_3to1 = df['ratio3'] == 1\n\n# Pairings: label, group1, group2\npairings = [\n    (\"2:1 vs 1:1\", df[is_2to1]['gave'].dropna(), df[is_1to1]['gave'].dropna()),\n    (\"3:1 vs 1:1\", df[is_3to1]['gave'].dropna(), df[is_1to1]['gave'].dropna()),\n    (\"3:1 vs 2:1\", df[is_3to1]['gave'].dropna(), df[is_2to1]['gave'].dropna())\n]\n\nfor label, group1, group2 in pairings:\n    mean1 = group1.mean()\n    mean2 = group2.mean()\n    mean_diff = mean1 - mean2\n    var1 = group1.var(ddof=1)\n    var2 = group2.var(ddof=1)\n    n1 = len(group1)\n    n2 = len(group2)\n\n    pooled_se = ((var1 / n1) + (var2 / n2)) ** 0.5\n    manual_t_stat = mean_diff / pooled_se\n    df_degrees = n1 + n2 - 2\n    critical_value = t.ppf(1 - alpha/2, df_degrees)\n    conclusion = \"Significant\" if abs(manual_t_stat) &gt; critical_value else \"Not Significant\"\n\n    results.append({\n        \"Comparison\": label,\n        \"Group 1 Mean Donation Rate\": round(mean1 * 100, 2),  # Convert to %\n        \"Group 2 Mean Donation Rate\": round(mean2 * 100, 2),\n        \"Mean Diff (pp)\": round(mean_diff * 100, 2),\n        \"T-Statistic\": round(manual_t_stat, 4),\n        \"Critical Value (±)\": round(critical_value, 4),\n        \"Conclusion\": conclusion\n    })\n\n# Display results\nimport pandas as pd\nresults1_df = pd.DataFrame(results)\nprint(results1_df.to_markdown(index=False))\n\n| Comparison   |   Group 1 Mean Donation Rate |   Group 2 Mean Donation Rate |   Mean Diff (pp) |   T-Statistic |   Critical Value (±) | Conclusion      |\n|:-------------|-----------------------------:|-----------------------------:|-----------------:|--------------:|---------------------:|:----------------|\n| 2:1 vs 1:1   |                         2.26 |                         2.07 |             0.19 |        0.965  |               1.9601 | Not Significant |\n| 3:1 vs 1:1   |                         2.27 |                         2.07 |             0.2  |        1.015  |               1.9601 | Not Significant |\n| 3:1 vs 2:1   |                         2.27 |                         2.26 |             0.01 |        0.0501 |               1.9601 | Not Significant |\n\n\n\n# Generate markdown table\nmarkdown_table = results1_df.to_markdown(index=False)\n\n# Read the original .qmd content\nwith open(\"index.qmd\", \"r\") as f:\n    qmd_text = f.read()\n\n# Replace the placeholder with the table\nqmd_text = qmd_text.replace(\"&lt;!-- INSERT_RESULTS1_HERE --&gt;\", markdown_table)\n\n# Write back the updated file\nwith open(\"index.qmd\", \"w\") as f:\n    f.write(qmd_text)\n\n\nAssess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision\n\n# Create the ratio1 variable\ndf['ratio1'] = ((df['ratio2'] == 0) & (df['ratio3'] == 0)).astype(int)\n\nX = df[['ratio1', 'ratio2', 'ratio3']]  # Independent variables\ny = df['gave']  # Dependent variable\n\nX = sm.add_constant(X)  # Add a constant term for the regression\nmodel = sm.OLS(y, X, missing='drop').fit()\n\nprint(model.summary())\n\ncoefficients = model.params\np_values = model.pvalues\n\nfor var in ['ratio1', 'ratio2', 'ratio3']:\n    if p_values[var] &lt; 0.05:\n        print(f\"The coefficient for {var} is {coefficients[var]:.4f}, which is statistically significant (p-value: {p_values[var]:.4f}).\")\n    else:\n        print(f\"The coefficient for {var} is {coefficients[var]:.4f}, which is not statistically significant (p-value: {p_values[var]:.4f}).\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     2.743\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0415\nTime:                        19:24:41   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.321e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       1.229e+10   1.54e+11      0.080      0.936   -2.89e+11    3.14e+11\nratio1     -1.229e+10   1.54e+11     -0.080      0.936   -3.14e+11    2.89e+11\nratio2     -1.229e+10   1.54e+11     -0.080      0.936   -3.14e+11    2.89e+11\nratio3     -1.229e+10   1.54e+11     -0.080      0.936   -3.14e+11    2.89e+11\n==============================================================================\nOmnibus:                    59815.676   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317583.266\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                     5.80e+14\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.13e-25. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\nThe coefficient for ratio1 is -12293201518.4371, which is not statistically significant (p-value: 0.9363).\nThe coefficient for ratio2 is -12293201518.4334, which is not statistically significant (p-value: 0.9363).\nThe coefficient for ratio3 is -12293201518.4333, which is not statistically significant (p-value: 0.9363).\n\n\n\n#The results show clear signs of a severe multicollinearity or design matrix error, likely caused by including all three dummy variables (ratio1, ratio2, ratio3) without dropping a reference group. Since the three dummies are mutually exclusive and exhaustive (they sum to 1), including all of them with a constant creates a perfect linear dependency, which leads to a singular matrix and unreliable coefficient estimates. \n# To fix this, I omit one of the ratio dummies (e.g., drop ratio1) so that the remaining coefficients (ratio2 and ratio3) are interpreted relative to the 1:1 match group. \n\nimport statsmodels.api as sm\n\n# Make sure 'gave' is numeric\ny = df['gave'].astype(float)\n\n# Only include ratio2 and ratio3 — 1:1 (ratio1) is the reference group\nX = df[['ratio2', 'ratio3']]\n\n# Add a constant (intercept)\nX = sm.add_constant(X)\n\n# Fit the linear regression model\nmodel = sm.OLS(y, X, missing='drop').fit()\n\n# Print summary\nprint(model.summary())\n\n# Interpret the coefficients\ncoefficients = model.params\np_values = model.pvalues\n\nfor var in ['ratio2', 'ratio3']:\n    if p_values[var] &lt; 0.05:\n        print(f\"The coefficient for {var} is {coefficients[var]:.4f}, which is statistically significant (p-value: {p_values[var]:.4f}).\")\n    else:\n        print(f\"The coefficient for {var} is {coefficients[var]:.4f}, which is not statistically significant (p-value: {p_values[var]:.4f}).\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.117\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0163\nTime:                        19:24:41   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.323e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0190      0.001     22.306      0.000       0.017       0.021\nratio2         0.0036      0.002      2.269      0.023       0.000       0.007\nratio3         0.0037      0.002      2.332      0.020       0.001       0.007\n==============================================================================\nOmnibus:                    59815.856   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317637.927\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nThe coefficient for ratio2 is 0.0036, which is statistically significant (p-value: 0.0233).\nThe coefficient for ratio3 is 0.0037, which is statistically significant (p-value: 0.0197).\n\n\n\nresults_r = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.values,\n    \"Standard Error\": model.bse.values,\n    \"t-Statistic\": model.tvalues.values,\n    \"p-Value\": model.pvalues.values,\n    \"CI Lower (95%)\": model.conf_int()[0].values,\n    \"CI Upper (95%)\": model.conf_int()[1].values\n})\n\n\nprint(results_r.to_markdown(index=False))\nmarkdown_table = results_r.to_markdown(index=False)\n\n# Read the original .qmd content\nwith open(\"index.qmd\", \"r\") as f:\n    qmd_text = f.read()\n\n# Replace the placeholder with the table\nqmd_text = qmd_text.replace(\"&lt;!-- INSERT_RESULTSr_HERE --&gt;\", markdown_table)\n\n# Write back the updated file\nwith open(\"index.qmd\", \"w\") as f:\n    f.write(qmd_text)\n\n| Variable   |   Coefficient |   Standard Error |   t-Statistic |      p-Value |   CI Lower (95%) |   CI Upper (95%) |\n|:-----------|--------------:|-----------------:|--------------:|-------------:|-----------------:|-----------------:|\n| const      |    0.0190151  |      0.000852479 |      22.3056  | 1.11719e-109 |      0.0173442   |       0.020686   |\n| ratio2     |    0.00361828 |      0.00159454  |       2.26917 | 0.023262     |      0.000492971 |       0.00674359 |\n| ratio3     |    0.0037183  |      0.00159479  |       2.33153 | 0.0197294    |      0.000592493 |       0.00684411 |\n\n\n\n\nCalculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n# Identify match groups\nis_1to1 = (df['treatment'] == 1) & (df['ratio2'] == 0) & (df['ratio3'] == 0)\nis_2to1 = df['ratio2'] == 1\nis_3to1 = df['ratio3'] == 1\n\n# donation rates\nrate_1to1 = df[is_1to1]['gave'].mean()\nrate_2to1 = df[is_2to1]['gave'].mean()\nrate_3to1 = df[is_3to1]['gave'].mean()\n\ndiff_21_vs_11 = rate_2to1 - rate_1to1\ndiff_31_vs_21 = rate_3to1 - rate_2to1\n\nprint(f\"2:1 vs 1:1 response rate difference: {diff_21_vs_11:.4f}\")\nprint(f\"3:1 vs 2:1 response rate difference: {diff_31_vs_21:.4f}\")\n\n# difference from coefficients from the regression\ncoef_2to1 = coefficients['ratio2']\ncoef_3to1 = coefficients['ratio3']\n\ndiff_coef_1to1_2to1 = coef_2to1  # Coefficient for ratio2 represents the difference from 1:1\ndiff_coef_2to1_3to1 = coef_3to1 - coef_2to1  # Difference between coefficients for ratio3 and ratio2\n\nprint(f\"Response rate difference from coefficients (2:1 - 1:1): {diff_coef_1to1_2to1:.4f}\")\nprint(f\"Response rate difference from coefficients (3:1 - 2:1): {diff_coef_2to1_3to1:.4f}\")\n\n2:1 vs 1:1 response rate difference: 0.0019\n3:1 vs 2:1 response rate difference: 0.0001\nResponse rate difference from coefficients (2:1 - 1:1): 0.0036\nResponse rate difference from coefficients (3:1 - 2:1): 0.0001\n\n\n\n\n\nSize of Charitable Contribution\n\nCalculate a t-test of the donation amount on the treatment status\nIncluding donation values where gave = 0 i.e. donation = 0\n\nfrom scipy.stats import t\n\n# Select donation amount by group\ntreatment_group_amount = df[df['treatment'] == 1]['amount'].dropna()\ncontrol_group_amount = df[df['treatment'] == 0]['amount'].dropna()\n\n# Basic stats\nmean_diff = treatment_group_amount.mean() - control_group_amount.mean()\nvar_treatment = treatment_group_amount.var(ddof=1)\nvar_control = control_group_amount.var(ddof=1)\nn_treatment = len(treatment_group_amount)\nn_control = len(control_group_amount)\n\n# Pooled standard error\npooled_se = ((var_treatment / n_treatment) + (var_control / n_control)) ** 0.5\n\n# Manual t-statistic\nmanual_t_stat = mean_diff / pooled_se\nprint(f\"Manual T-statistic: {manual_t_stat:.4f}\")\n\n# Degrees of freedom (for equal variance t-test)\ndf_degrees = n_treatment + n_control - 2\n\n# Critical t value for 95% confidence (two-tailed)\ncritical_value = t.ppf(1 - 0.025, df_degrees)\nprint(f\"Critical value (±): {critical_value:.4f}\")\n\n# Conclusion\nif abs(manual_t_stat) &gt; critical_value:\n    print(\"Result: Statistically significant difference in donation amount.\")\nelse:\n    print(\"Result: No statistically significant difference in donation amount.\")\n\nManual T-statistic: 1.9182\nCritical value (±): 1.9600\nResult: No statistically significant difference in donation amount.\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Extract donation amounts for treatment and control groups\ntreatment_donations = df[df['treatment'] == 1]['amount'].dropna()\ncontrol_donations = df[df['control'] == 1]['amount'].dropna()\n\n# Perform t-test\nt_stat, p_value = ttest_ind(treatment_donations, control_donations, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nmean_treatment = treatment_donations.mean()\nmean_control = control_donations.mean()\nprint(f\"Mean donation (treatment group): ${mean_treatment:.2f}\")\nprint(f\"Mean donation (control group):   ${mean_control:.2f}\")\n\n# Check significance at 95% confidence level\nif p_value &lt; 0.05:\n    print(\"The difference in donation amounts between treatment and control groups is statistically significant.\")\nelse:\n    print(\"The difference in donation amounts between treatment and control groups is not statistically significant.\")\n\nT-statistic: 1.9183\nP-value: 0.0551\nMean donation (treatment group): $0.97\nMean donation (control group):   $0.81\nThe difference in donation amounts between treatment and control groups is not statistically significant.\n\n\nOnly including donation values where gave = 1 i.e. donation &gt; 0\n\nfrom scipy.stats import ttest_ind\n\n# Extract donation amounts for treatment and control groups\ntreatment_donations = df[(df['treatment'] == 1) & (df['gave'] == 1)]['amount'].dropna()\ncontrol_donations = df[(df['control'] == 1) & (df['gave'] == 1)]['amount'].dropna()\n\n# Perform t-test\nt_stat, p_value = ttest_ind(treatment_donations, control_donations, equal_var=False)\n\nprint(f\"T-statistic: {t_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nmean_treatment = treatment_donations.mean()\nmean_control = control_donations.mean()\nprint(f\"Mean donation (treatment group): ${mean_treatment:.2f}\")\nprint(f\"Mean donation (control group):   ${mean_control:.2f}\")\n\n# Check significance at 95% confidence level\nif p_value &lt; 0.05:\n    print(\"The difference in donation amounts between treatment and control groups is statistically significant.\")\nelse:\n    print(\"The difference in donation amounts between treatment and control groups is not statistically significant.\")\n\nT-statistic: -0.5846\nP-value: 0.5590\nMean donation (treatment group): $43.87\nMean donation (control group):   $45.54\nThe difference in donation amounts between treatment and control groups is not statistically significant.\n\n\n\n#using bivariate regression \n\ndonors = df[df['gave'] == 1].copy()\n\ny = donors['amount'].astype(float)\nX = sm.add_constant(donors['treatment'])  # Treatment: 1 = treatment group, 0 = control\n\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\ncoef = model.params['treatment']\np_val = model.pvalues['treatment']\nmean_treat = donors[donors['treatment'] == 1]['amount'].mean()\nmean_control = donors[donors['treatment'] == 0]['amount'].mean()\n\nprint(f\"Mean donation (treatment group): ${mean_treat:.2f}\")\nprint(f\"Mean donation (control group):   ${mean_control:.2f}\")\nprint(f\"Coefficient (treatment): {coef:.4f}, p-value: {p_val:.4f}\")\n\nif p_val &lt; 0.05:\n    print(\"The treatment has a statistically significant effect on donation amount (among donors).\")\nelse:\n    print(\"There is no statistically significant difference in donation amount between treatment and control groups (among donors).\")\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):              0.561\nTime:                        19:24:41   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nMean donation (treatment group): $43.87\nMean donation (control group):   $45.54\nCoefficient (treatment): -1.6684, p-value: 0.5615\nThere is no statistically significant difference in donation amount between treatment and control groups (among donors).\n\n\n\nresults_d = pd.DataFrame({\n    \"Variable\": model.params.index,\n    \"Coefficient\": model.params.values,\n    \"Standard Error\": model.bse.values,\n    \"t-Statistic\": model.tvalues.values,\n    \"p-Value\": model.pvalues.values,\n    \"CI Lower (95%)\": model.conf_int()[0].values,\n    \"CI Upper (95%)\": model.conf_int()[1].values\n})\n\n\nprint(results_d.to_markdown(index=False))\nmarkdown_table = results_d.to_markdown(index=False)\n\n# Read the original .qmd content\nwith open(\"index.qmd\", \"r\") as f:\n    qmd_text = f.read()\n\n# Replace the placeholder with the table\nqmd_text = qmd_text.replace(\"&lt;!-- INSERT_RESULTSd_HERE --&gt;\", markdown_table)\n\n# Write back the updated file\nwith open(\"index.qmd\", \"w\") as f:\n    f.write(qmd_text)\n\n| Variable   |   Coefficient |   Standard Error |   t-Statistic |     p-Value |   CI Lower (95%) |   CI Upper (95%) |\n|:-----------|--------------:|-----------------:|--------------:|------------:|-----------------:|-----------------:|\n| const      |      45.5403  |          2.42338 |     18.7921   | 5.47358e-68 |         40.785   |         50.2956  |\n| treatment  |      -1.66839 |          2.87238 |     -0.580839 | 0.561476    |         -7.30477 |          3.96799 |\n\n\n\n# Check if control = 1 for all rows where treatment = 0\nall_control_correct = (df[df['treatment'] == 0]['control'] == 1).all()\n\nif all_control_correct:\n    print(\"For all rows where treatment = 0, control = 1.\")\nelse:\n    print(\"There are rows where treatment = 0 but control is not 1.\")\n\nFor all rows where treatment = 0, control = 1.\n\n\n\n# Check if treatment = 1 for all rows where control = 0\nall_control_correct = (df[df['control'] == 0]['treatment'] == 1).all()\n\nif all_control_correct:\n    print(\"For all rows where c = 0, t = 1.\")\nelse:\n    print(\"There are rows where c = 0 but t is not 1.\")\n\nFor all rows where c = 0, t = 1.\n\n\n\n\nMake two plots: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot\n\n# Filter donation amounts for treatment and control groups where donations were made (gave = 1)\ntreatment_donations = df[(df['treatment'] == 1) & (df['gave'] == 1)]['amount']\ncontrol_donations = df[(df['control'] == 1) & (df['gave'] == 1)]['amount']\n\n# Calculate means for annotation\nmean_treatment = treatment_donations.mean()\nmean_control = control_donations.mean()\n\n# Plot for treatment group\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.histplot(treatment_donations, bins=30, kde=False, color='blue', alpha=0.7)\nplt.axvline(mean_treatment, color='red', linestyle='--', label=f'Mean: ${mean_treatment:.2f}')\nplt.title('Donation Amounts - Treatment Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.ylim(0, 300)  # Set y-axis scale\nplt.legend()\n\n# Plot for control group\nplt.subplot(1, 2, 2)\nsns.histplot(control_donations, bins=30, kde=False, color='green', alpha=0.7)\nplt.axvline(mean_control, color='red', linestyle='--', label=f'Mean: ${mean_control:.2f}')\nplt.title('Donation Amounts - Control Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\nplt.ylim(0, 300)  # Set y-axis scale\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSimulation Experiment\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n\nLaw of Large Numbers\nSimulating 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. I then calculate a vector of 10,000 differences, and then plot the cumulative average of that vector of differences.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# True probabilities\np_control = 0.018 #do not get any charitable match\np_treatment = 0.022 #get a charitable match of any size\n\nn = 10000\n\n# Simulate 10,000 Bernoulli draws from each group\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\n# Compute differences between each draw (pairwise)\ndifferences = treatment - control  # each element is 0, +1, or -1\n\n# Cumulative average of the differences\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n + 1)\n\n\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff, label=\"Cumulative Average of Differences\", color='blue')\nplt.axhline(0.004, color='red', linestyle='--', label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.title(\"Simulation Demonstrating Law of Large Numbers\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis average is “noisey” when only averaging a few numbers, but “settles down” and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. But in this run, it converges slightly above the true treatment effect. Let’s try another run.\n\n\n\n# True probabilities\np_control = 0.018 #do not get any charitable match\np_treatment = 0.022 #get a charitable match of any size\n\nn = 10000\n\n# Simulate 10,000 Bernoulli draws from each group\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\n# Compute differences between each draw (pairwise)\ndifferences = treatment - control  # each element is 0, +1, or -1\n\n# Cumulative average of the differences\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n + 1)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff, label=\"Cumulative Average of Differences\", color='blue')\nplt.axhline(0.004, color='red', linestyle='--', label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.title(\"Simulation Demonstrating Law of Large Numbers\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nI make 4 histograms at sample sizes 50, 200, 500, and 1000.\nTo do this for a sample size of e.g. 50, I take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws.\nThen, I repeat that process 999 more times so that I have 1000 averages. Lastly, I plot the histogram of those averages, repeating for the other 3 histograms.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Create subplots\nfig, axs = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    \n    for _ in range(num_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(diff)\n    \n    # Plot histogram\n    axs[i].hist(mean_diffs, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(0.004, color='red', linestyle='--', label='True Treatment Effect')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg. Treatment Effect\")\n    axs[i].legend()\n\naxs[0].set_ylabel(\"Frequency\")\nplt.suptitle(\"Demonstrating the Central Limit Theorem\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()"
  },
  {
    "objectID": "projects/project 1/index.html",
    "href": "projects/project 1/index.html",
    "title": "Predicting Late Delivery Risk for a Sport Retail Business",
    "section": "",
    "text": "This project analyzes and predicts late delivery trends in a sports retail business, identifying key factors like shipping mode, order location, and payment method using logistic regression. It provides actionable insights via interactive dashboards to improve delivery performance.\nGithub Link here.\n\n\nThis project, titled “Identifying Risk of Late Deliveries in a Sports Retail Business,” focuses on analyzing and mitigating late deliveries in a sports retail business operating across the Americas. The business fulfills over 21,000 orders annually, generating $42 million in sales, with products sourced from the USA and Puerto Rico and sold across 22 countries.\nSkills used: Python, Streamlit, Data visualization, Scikit-learn, Feature Engineering, RandomForest, Logistic Regression"
  },
  {
    "objectID": "projects/project 1/index.html#sub-header",
    "href": "projects/project 1/index.html#sub-header",
    "title": "Predicting Late Delivery Risk for a Sport Retail Business",
    "section": "",
    "text": "This project, titled “Identifying Risk of Late Deliveries in a Sports Retail Business,” focuses on analyzing and mitigating late deliveries in a sports retail business operating across the Americas. The business fulfills over 21,000 orders annually, generating $42 million in sales, with products sourced from the USA and Puerto Rico and sold across 22 countries.\nSkills used: Python, Streamlit, Data visualization, Scikit-learn, Feature Engineering, RandomForest, Logistic Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nitya Kowsalya Vootla",
    "section": "",
    "text": "Call me the KPI Knight 🦸🏿‍♀️I achieve business goals through strategic insights, scalable solutions, and cross-functional execution. I build platforms, analytical tools, and products hat help businesses optimize processes, mitigate risks, enhance customer experience, and drive sustainable growth.\n🦸🏿‍♀️At TikTok, I launched a risk intelligence platform to ensure a safer consumer shopping experience and make uncovering risks a 50% more efficient process across 12 global teams.\n🦸🏿‍♀️At Glints, a high-growth Southeast Asian startup, I spearheaded the implementation of CRM software, increasing lead conversion by 40%, and launched a B2B client service ticketing system that improved customer issue resolution by 52%.\n🦸🏿‍♀️At Halliburton, I built more efficient project management tools that optimized supply chain analytics and tracking processes, achieving $200K in cost savings within my first six months in a full-time role.\n🦸🏿‍♀️ At UCSD, I work extensively on building and tuning LLM powered Assistants, as well as advanced statistical modeling and machine learning methods to optimize fraud analytics and customer analytics.\nArmed with a Master’s in Business Analytics from UC San Diego, I specialize in translating complex data into actionable strategies to build tools and products. Whether it’s driving customer satisfaction, refining operational efficiency, or scaling analytics capabilities, I bring resilience and a builder’s precision to every project. Beyond my work, I enjoy volunteering as a student consultant for non-profits, running, and creating vegan dishes for my friends.\nLet’s connect if you’re looking for a results-driven, data-savvy creative to augment your platforms and products."
  },
  {
    "objectID": "projects/project 2/index.html",
    "href": "projects/project 2/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\n\n\nIn their 2007 study, Karlan and List implemented a large-scale natural field experiment to investigate how the match rate of matching grants influence charitable giving behavior. They employ a direct mail solicitation to explore whether and the extent to which, “price” i.e. cost to the donor of providing one dollar’s worth of public good to a nonprofit, is crucial in charitable fundraising.\nThe experiment involved 50,083 previous donors who had contributed, at least once since 1991, to a chosen liberal nonprofit organization in the United States. All individuals received one of the variations of a four page fundraising letter and reply card via direct mail, a standard channel used large charities in the US to ensure practical interest and high external validity. Furthermore, because the organization is politically engaged, this allows the team to explore heterogeneous treatment effects, particularly in how donors from “red” and “blue” states responded to the match offers.\nParticipants were randomly assigned to either a control group (approximately one-third of the sample) or a treatment group (the remaining two-thirds). The control group received a letter following the organization’s conventional format and a reply card with a large logo of the organization, while the treatment group received a letter that included an announcement on a “concerned fellow member” offering to match their donation, with a reply card that included the details of the match in bold font.\nWithin the treatment group, the experiment further varied the characteristics of the matching grant along three dimensions with equal probability:\n\nFirst, the matching ratio was randomized to be either $1:$1, $2:$1, or $3:$1, meaning that for each dollar donated, the organization would receive an additional $1, $2, or $3, respectively. This allowed the researchers to test whether increasing the perceived “value” of a donation would lead to higher giving.\nSecond, the maximum size of the matching grant pool was varied, with donors being told it was capped at $25,000, $50,000, $100,000, or left unstated. This tested whether the size of the match fund—possibly perceived as a signal of the urgency or importance of the campaign—influenced donor behavior.\nThird, the suggested donation amount was tailored to each donor’s prior giving history, using either the same amount as their highest previous contribution, or scaling it up by 1.25 or 1.5 times. The chosen example was used in the matching paragraph to illustrate the impact of their gift under the match.\n\nThe design of the experiment was intended not only to evaluate the overall impact of matching grants but also to probe how different configurations of the matching appeal might influence giving, by understanding the social behavioral mechanisms behind charitable donations like conformity, social norms, and reciprocity.\nThe key hypotheses in this experiment are:\n\nPresence of a Matching Grant Increases Giving: Donors are more likely to contribute when a matching grant is offered, compared to when no match is offered.\nHigher Match Ratios Lead to Higher Giving: Increasing the match ratio (e.g., from $1:$1 to $2:$1 or $3:$1) will further increase donation rates and total contributions by making the “price” of giving lower.\nLarger Matching Grant Pools Increase Giving: Announcing a larger maximum size of the matching fund (e.g., $100,000 vs. $25,000) will lead to greater donations due to perceived urgency, credibility, or impact.\nHigher Suggested Donation Amounts Influence Giving: Presenting donors with higher suggested donation amounts will lead to larger contributions, possibly by anchoring their perception of a “typical” or expected gift.\nDonor Responsiveness to Matching Grants Varies by Political Context: Donors living in different political environments (e.g., red vs. blue states) will respond differently to the match offer, potentially due to identity, perceived relevance, or local norms.\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project 2/index.html#introduction",
    "href": "projects/project 2/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\n\n\nIn their 2007 study, Karlan and List implemented a large-scale natural field experiment to investigate how the match rate of matching grants influence charitable giving behavior. They employ a direct mail solicitation to explore whether and the extent to which, “price” i.e. cost to the donor of providing one dollar’s worth of public good to a nonprofit, is crucial in charitable fundraising.\nThe experiment involved 50,083 previous donors who had contributed, at least once since 1991, to a chosen liberal nonprofit organization in the United States. All individuals received one of the variations of a four page fundraising letter and reply card via direct mail, a standard channel used large charities in the US to ensure practical interest and high external validity. Furthermore, because the organization is politically engaged, this allows the team to explore heterogeneous treatment effects, particularly in how donors from “red” and “blue” states responded to the match offers.\nParticipants were randomly assigned to either a control group (approximately one-third of the sample) or a treatment group (the remaining two-thirds). The control group received a letter following the organization’s conventional format and a reply card with a large logo of the organization, while the treatment group received a letter that included an announcement on a “concerned fellow member” offering to match their donation, with a reply card that included the details of the match in bold font.\nWithin the treatment group, the experiment further varied the characteristics of the matching grant along three dimensions with equal probability:\n\nFirst, the matching ratio was randomized to be either $1:$1, $2:$1, or $3:$1, meaning that for each dollar donated, the organization would receive an additional $1, $2, or $3, respectively. This allowed the researchers to test whether increasing the perceived “value” of a donation would lead to higher giving.\nSecond, the maximum size of the matching grant pool was varied, with donors being told it was capped at $25,000, $50,000, $100,000, or left unstated. This tested whether the size of the match fund—possibly perceived as a signal of the urgency or importance of the campaign—influenced donor behavior.\nThird, the suggested donation amount was tailored to each donor’s prior giving history, using either the same amount as their highest previous contribution, or scaling it up by 1.25 or 1.5 times. The chosen example was used in the matching paragraph to illustrate the impact of their gift under the match.\n\nThe design of the experiment was intended not only to evaluate the overall impact of matching grants but also to probe how different configurations of the matching appeal might influence giving, by understanding the social behavioral mechanisms behind charitable donations like conformity, social norms, and reciprocity.\nThe key hypotheses in this experiment are:\n\nPresence of a Matching Grant Increases Giving: Donors are more likely to contribute when a matching grant is offered, compared to when no match is offered.\nHigher Match Ratios Lead to Higher Giving: Increasing the match ratio (e.g., from $1:$1 to $2:$1 or $3:$1) will further increase donation rates and total contributions by making the “price” of giving lower.\nLarger Matching Grant Pools Increase Giving: Announcing a larger maximum size of the matching fund (e.g., $100,000 vs. $25,000) will lead to greater donations due to perceived urgency, credibility, or impact.\nHigher Suggested Donation Amounts Influence Giving: Presenting donors with higher suggested donation amounts will lead to larger contributions, possibly by anchoring their perception of a “typical” or expected gift.\nDonor Responsiveness to Matching Grants Varies by Political Context: Donors living in different political environments (e.g., red vs. blue states) will respond differently to the match offer, potentially due to identity, perceived relevance, or local norms.\n\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project 2/index.html#data",
    "href": "projects/project 2/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset comprises over 50,000 individual-level observations from a natural field experiment testing how matching grants affect charitable giving. Each observation represents a prior donor to a politically liberal nonprofit organization in the U.S., who received a direct mail solicitation in 2005. The data include detailed treatment indicators, such as whether the individual received a matching grant (treatment), the match ratio (1:1, 2:1, or 3:1), the maximum match pool size (e.g., $25k, $50k), and suggested donation amounts based on their past giving. A tabulated description of the data variables id provided below, followed by a more detailed description:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code"
  },
  {
    "objectID": "projects/project 2/index.html#data-description-stats",
    "href": "projects/project 2/index.html#data-description-stats",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data Description Stats",
    "text": "Data Description Stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nask1\nask2\nask3\namount\ngave\namountchange\nhpa\nfreq\nyears\nmrm2\nnonlit\ncases\nstatecnt\nstateresponse\nstateresponset\nstateresponsec\nstateresponsetminc\nperbush\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n50083\n50083\n50083\n50083\n50083\n50083\n50083\n50083\n50083\n50083\n50082\n50082\n49631\n49631\n50083\n50083\n50083\n50080\n50080\n50048\n48217\n48047\n48217\n48221\n48209\n48214\n48215\n48217\n\n\n0.666813\n0.333187\n71.5018\n91.7927\n111.046\n0.915694\n0.0206457\n-52.672\n59.385\n8.03935\n6.09754\n13.0073\n2.47392\n1.49977\n5.99882\n0.0206269\n0.0219885\n0.0177167\n0.00427312\n0.48794\n0.819599\n0.0867098\n0.321694\n2.42901\n54815.7\n0.669418\n0.391661\n0.871968\n\n\n0.471357\n0.471357\n101.729\n127.253\n151.674\n8.70739\n0.142197\n1267.1\n71.1799\n11.3945\n5.50349\n12.0814\n1.96153\n1.15514\n5.74599\n0.0051708\n0.00625721\n0.00751621\n0.00911209\n0.0787327\n0.16856\n0.135868\n0.103039\n0.378105\n22027.3\n0.193405\n0.186599\n0.258633\n\n\n0\n0\n25\n35\n50\n0\n0\n-200412\n0\n0\n0\n0\n0\n0\n0.00199481\n0\n0\n0\n-0.047619\n0.0909091\n0.00941798\n0\n0\n0\n5000\n0\n0\n0\n\n\n0\n0\n35\n45\n55\n0\n0\n-50\n30\n2\n2\n4\n1\n1\n1.83323\n0.0181635\n0.0184932\n0.0128617\n-0.00138826\n0.444444\n0.755845\n0.0147292\n0.258311\n2.21\n39181\n0.560222\n0.235647\n0.884929\n\n\n1\n0\n45\n60\n70\n0\n0\n-30\n45\n4\n5\n8\n3\n1\n3.5388\n0.0197095\n0.0216972\n0.0198814\n0.00177869\n0.484848\n0.872797\n0.0365541\n0.305534\n2.44\n50673\n0.712296\n0.373744\n1\n\n\n1\n1\n65\n85\n100\n0\n0\n-25\n60\n10\n9\n19\n4\n2\n9.60702\n0.0230482\n0.0247027\n0.0208062\n0.0105448\n0.525253\n0.938827\n0.090882\n0.369132\n2.66\n66005\n0.816798\n0.530036\n1\n\n\n1\n1\n1500\n1875\n2250\n400\n1\n275\n1000\n218\n95\n168\n6\n4\n17.3688\n0.0769231\n0.111111\n0.0526316\n0.111111\n0.731959\n1\n0.989622\n0.997544\n5.27\n200001\n1\n1\n1\n\n\n\n\nThe donation rate across the sample was around 2.1%, and the average change in donation amount was a decrease of $52.67, reflecting the typical non-responsiveness of most individuals.\nDonor histories show that the average highest prior contribution was about $59.39, and the typical donor had given eight times over six years, with about 13 months since their last gift. Roughly 49% were small prior donors (last gift under $35), and 51% had been donors for five or more years, suggesting a base of moderately engaged supporters. The donor base was predominantly male—only 28% of donors were female—and about 9% were identified as couples, which could potentially reflect joint giving decisions or household-level donation behavior.\nIn terms of geographic and political context, 40% of donors were from red states, 60% from blue states, and about half lived in red counties. The average vote share for George W. Bush in the donors’ states was just under 49%, with around 18.5% living in politically competitive states (close25). The treatment increased the average response rate by about 0.43 percentage points, with the treated group’s response rate at 2.2%, compared to 1.8% in the control. Additionally, the nonprofit was moderately active in most states, averaging about 2.47 nonlitigation events (public education campaigns, policy advocacy or lobbying, community organizing public events or speaking engagements etc.) and 1.5 legal cases per state, offering a measure of the organization’s local visibility.\nFinally, the dataset includes zip-code-level demographics that add further context to donor behavior. On average, zip codes were 82% white, with about 32% of the population aged 18–39. The median household income was approximately $54,816, and about 67% of residents were homeowners. Nearly 39% had at least a bachelor’s degree and 87% lived in urban areas. Majority of the donors were male (72.2%). These contextual data allow for analysis of how local socioeconomic factors interact with donor responses to the match offer, enriching the study’s implications for both behavioral economics and nonprofit strategy.\n\nDistribution and Frequency Plots of Donation Amount from donors who gave based on past donation and financial status data and variables being modified\n\nThe distributions of key variables reveal that most donors in the experiment were low-frequency, small-dollar contributors, with past donations typically under $100 and fewer than 10 total gifts. While a few donors were highly engaged or generous, they were the exception. The years since first donation were spread relatively evenly, suggesting a mix of both new and long-term supporters. Most recent donations occurred within the last few months, indicating a strong recency effect. Donors primarily lived in zip codes with median household incomes between $30,000 and $70,000, though some resided in wealthier areas.\nOn the experimental side, the match ratio, match threshold, and suggested donation amount were all well balanced across groups, with no major disparities—indicating successful randomization. These distributions support the validity of comparisons across treatment groups and suggest that variables like recency of donation and donor longevity may be strong predictors of giving behavior.\n\n\nDistribution and Frequency Plots of Donation Amount from donors who gave based on donor characteristics, geopolitical context, and location (zipcode) demographics\n\nThe visualizations show how donation amounts vary across donor characteristics, political context, and zip-code demographics. Donors who were previously small contributors (ltmedmra = 1) or had donated for over 5 years (year5 = 1) gave slightly more on average. Male donors gave more than female donors, and donors from blue states (red0 = 0) gave higher amounts than those from red states. Donation amounts increased with moderate values of Bush vote share (perbush) but not at extremes. Donations also varied non-linearly with the number of non-litigation activities in a state (nonlit). From a demographic perspective, donation amounts peaked among donors from zip codes with moderate income levels, higher homeownership (powner), and moderately educated areas (psch_atlstba), suggesting that both neighborhood affluence and stability may influence giving behavior.\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another."
  },
  {
    "objectID": "projects/project 2/index.html#experimental-results",
    "href": "projects/project 2/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nThe proportion of donors who made a donation in the treatment group was 2.2%, compared to 1.8% in the control group, wchih seems roughly equivalent. However, this also suggests that the matching grant increased the likelihood of making a donation by about 0.4 percentage points, or roughly 22% relative to the control group. It is also to be noted that participants were randomly assigned to either a control group (approximately one-third of the sample) or a treatment group (the remaining two-thirds), suggesting that the treatment group may have more statistical power to detect differences in the response rate.\n\nImpact of Matching Gifts on Donation Likelihood: Evidence from a Bivariate Regression\nNext in my analysis, I ran a bivariate linear regression to test whether being in the treatment group—receiving a letter that included a matching donation offer—increased the likelihood of making a charitable donation. Using the binary outcome variable gave, I found that being in the treatment group increased the probability of donating by approximately 0.42 percentage points (coefficient 0.0042), keeping all else constant, while the intercept (0.0179) tells us the predicted probability of donating for someone in the control group. This result was statistically significant (p = 0.0019), indicating a real and measurable effect that is unlikely due to chance. The results are documented here:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStandard Error\nt-Statistic\np-Value\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\nconst\n0.0178582\n0.00110068\n16.2246\n4.77903e-59\n0.0157009\n0.0200156\n\n\ntreatment\n0.00418035\n0.00134791\n3.10136\n0.0019274\n0.00153844\n0.00682227\n\n\n\nMy findings closely match the descriptive statistics reported in Table 2A, Panel A of the paper, which shows a response rate of 2.2% for the treatment group versus 1.8% for the control group—also a 0.4 percentage point difference. While small in absolute terms, this represents a 22% relative increase in giving behavior, suggesting that individuals are more likely to donate when they believe their contribution will have greater impact. This supports the broader conclusion of the study: that subtle nudges, like matching gifts, can meaningfully influence human behavior by increasing the perceived value of giving. The result reinforces the idea that charitable behavior is not just about fixed generosity levels, it’s also about how the opportunity to give is presented.\n\n\nTreatment or Control Group Assignment Effect on Donor Behavior: Probit Evidence from a Charitable Giving Experiment\nFollowing this, I ran a Probit regression to examine whether being assigned to the treatment group (receiving a fundraising letter that included a matching donation offer) or control group affected the likelihood of making a charitable donation. The model found a positive and statistically significant treatment effect, with a coefficient of 0.0868 and a p-value of 0.0019, indicating the result is statistically significant, and unlikely due to chance.While the coefficient itself isn’t directly interpretable in percentage points (as it is in linear regression), the positive sign means that individuals in the treatment group were more likely to donate than those in the control group. The results are as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStandard Error\nt-Statistic\np-Value\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\nconst\n0.0178582\n0.00110068\n16.2246\n4.77903e-59\n0.0157009\n0.0200156\n\n\ntreatment\n0.00418035\n0.00134791\n3.10136\n0.0019274\n0.00153844\n0.00682227\n\n\n\nThis aligns with earlier findings from the linear regression and confirms the descriptive results shown in Table 2A, where donation rates were higher in the treatment group.\nTherefore, this result reinforces that people who received a letter offering a matching donation were more likely to give than those who did not. While the actual increase in probability isn’t directly visible from the Probit coefficient, the positive and significant effect confirms that the matching offer changed behavior. This suggests that people are responsive to how giving is framed, in this case, making their contribution feel more impactful through a match. Even when the baseline donation rate is low, a small psychological nudge like a match can increase engagement and drive meaningful behavior change.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions.Probit regression results don’t match Table likely, because that table reports marginal effects (the estimated change in probability of donating when moving from control to treatment) than the raw coefficients from a Probit model. However, my Probit regression includes only the basic treatment indicator and outputs the raw coefficient, not the marginal effect. In a linear probability model (ordinary least squares on a binary outcome), the coefficient directly estimates the marginal effect, which is why the linear regression results match table 3 column 1.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nUsing a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not\n\n\n\n\n\n\n\n\n\n\n\n\nComparison\nGroup 1 Mean Donation Rate\nGroup 2 Mean Donation Rate\nMean Diff (pp)\nT-Statistic\nCritical Value (±)\nConclusion\n\n\n\n\n2:1 vs 1:1\n2.26\n2.07\n0.19\n0.965\n1.9601\nNot Significant\n\n\n3:1 vs 1:1\n2.27\n2.07\n0.2\n1.015\n1.9601\nNot Significant\n\n\n3:1 vs 2:1\n2.27\n2.26\n0.01\n0.0501\n1.9601\nNot Significant\n\n\n\nTo test whether the size of the match ratio affected donation behavior, I conducted a series of t-tests comparing donation rates across 1:1, 2:1, and 3:1 match groups. The results show that while donation rates were slightly higher for the 2:1 (2.26%) and 3:1 (2.27%) match groups compared to the 1:1 group (2.07%), these differences were not statistically significant at the 95% confidence level. Similarly, there was no meaningful difference between the 3:1 and 2:1 groups. All t-statistics were well below the critical threshold of 1.96, and all p-values exceeded 0.05.\nThese findings suggest that simply increasing the generosity of the match offer, from 1:1 to 2:1 or 3:1, did not significantly increase the likelihood that someone would donate. In other words, while matching gifts appear to influence behavior overall, there is no strong evidence that larger match ratios (beyond 1:1) further increase donation rates. This directly aligns with authors’ statement in page 8 that larger match ratios (2:1, 3:1) did not have a meaningful influence on donation behavior, at least not in a statistically significant way. This confirms the idea that donors respond to the presence of a match offer, but are not particularly sensitive to how generous the match is.\n\n\nUsing a series of linear regressions to test whether the size of the match ratio has an effect on whether people donate or not\nI ran a linear regression to evaluate whether the size of the match ratio (1:1, 2:1, or 3:1) had an effect on the likelihood of making a donation (gave, a binary variable). The model included dummy variables ratio1, ratio2, and ratio3 as independent variables, representing each of the three match groups. “ratio1” was created by flagging the cases where raio2 and ratio3 are 0, in the treatment group. The regression included a constant term, so the effect of each ratio variable is measured relative to an implicit baseline (control group).\nIt is observed from the results of the linear regression that All three coefficients (ratio1, ratio2, and ratio3) are extremely large negative numbers (~-1.23e+10), and all are identical, which is a red flag. The standard errors are also enormous (~1.54e+11), resulting in very small t-statistics (≈ -0.08) and very high p-values (~0.936). Moreover, none of the coefficients are statistically significant, and the model R² is effectively 0, meaning the model explains none of the variation in donation behavior.\nThe results show clear signs of a severe multicollinearity or design matrix error, likely caused by including all three dummy variables (ratio1, ratio2, ratio3) without dropping a reference group. Since the three dummies are mutually exclusive and exhaustive (they sum to 1), including all of them with a constant creates a perfect linear dependency, which leads to a singular matrix and unreliable coefficient estimates. To fix this, I omit one of the ratio dummies (e.g., drop ratio1) so that the remaining coefficients (ratio2 and ratio3) are interpreted relative to the 1:1 match group.\nThe results now show the following:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStandard Error\nt-Statistic\np-Value\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\nconst\n0.0190151\n0.000852479\n22.3056\n1.11719e-109\n0.0173442\n0.020686\n\n\nratio2\n0.00361828\n0.00159454\n2.26917\n0.023262\n0.000492971\n0.00674359\n\n\nratio3\n0.0037183\n0.00159479\n2.33153\n0.0197294\n0.000592493\n0.00684411\n\n\n\n\nThe intercept (0.0190) indicates that the average donation rate in the 1:1 match group was approximately 1.9%.\n\nThe coefficient for ratio2 is 0.0036, which means that the 2:1 match group donated at a rate that was 0.36 percentage points higher than the 1:1 group. This effect is statistically significant at the 5% level (p = 0.023).\n\nThe coefficient for ratio3 is 0.0037, meaning the 3:1 match group donated 0.37 percentage points more than the 1:1 group. This is also statistically significant (p = 0.020).\n\nIt is also to be noted that the model R^ is effectively 0, indicating that the model explains almost none of the variation in donation behavior.\n\n\nThus, the regression provides evidence that larger match ratios do modestly increase the likelihood of donating, compared to a standard 1:1 match. While the increases (less than half a percentage point) are small in absolute terms, they are statistically significant, suggesting that donors do respond to more generous match offers, even if the effect size is limited. This complements earlier t-test findings and supports the idea that match generosity does matter, but only slightly.\n\n\nResponse rate difference between 1:1, 2:1, and 3:1 match ratios\nI now calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios, both directly from the data and from the differences in the fitted coefficients in the previous regression. The response rates are as follows:\n2:1 vs 1:1 response rate difference: 0.0019 3:1 vs 2:1 response rate difference: 0.0001 Response rate difference from coefficients (2:1 - 1:1): 0.0036 Response rate difference from coefficients (3:1 - 2:1): 0.0001\nThese results suggest that increasing the match ratio from 1:1 to 2:1 has a modest but meaningful effect on donor behavior (The 2:1 match group donated at a rate 0.19 percentage points higher as per the data, and 0.36 percentage points higher as per the difference from the coefficients, than the 1:1 group). However, increasing the match further from 2:1 to 3:1 offers almost no additional benefit (0.0001). Donors seem responsive to the presence and basic enhancement of a match, but not to increasingly generous match sizes beyond a certain point.\nNOTE: The small difference between your raw mean comparison and regression coefficient comes from how OLS regression estimates parameters (by minimizing squared errors)\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nT-test of donation amount on the treatment status\nFirst, I conduct a t-test to compare the average donation amount between individuals in the treatment group (who received a matching gift offer) and those in the control group (no match).\nThe analysis included all individuals, regardless of whether they donated or not. The Mean donation (treatment group) is $0.97 and the Mean donation (control group) is $0.81. The observed t-stat was 1.9182, at a p-value of 0.0551, which is statistically insignificant.While the treatment group gave slightly more on average, the p-value is just above the conventional 0.05 threshold, meaning the result is not statistically significant at the 95% confidence level.\nOverall, this analysis suggests that people who received a matching gift offer tended to donate more on average, but the evidence is not strong enough to confirm that this difference is due to the treatment rather than random chance. The p-value (0.055) is close to the significance cutoff, which means there might be a weak effect, but it’s not statistically conclusive.\n\n\nT-test of donation amount on the treatment status using only data from people who actually donated\nI also conducted an analysis including only individual from both group who donated (gave == 1). The mean donation among the treatment group was $43.87, while the control group gave slightly more on average, at $45.54. The observed t-statistic was -0.581 with a p-value of 0.561, which is well above the conventional 0.05 threshold. Similar to the analysis above, this means the difference is not statistically significant at the 95% confidence level, and we cannot conclude that the treatment had an effect on donation amount among donors.\nConfirming the t-stat and p-values using a bivariate regression of amount on treatment, conditional on donation, I took a deeper look at the regression coefficients. The regression coefficient for treatment is -1.67, meaning that, on average, donors in the treatment group gave $1.67 less than those in the control group, keeping all else constant. However, this difference is not statistically significant (p = 0.561), and the confidence interval ranges from about –$7.31 to +$3.97, meaning the true effect could easily be zero or even slightly positive. The results are as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStandard Error\nt-Statistic\np-Value\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\nconst\n45.5403\n2.42338\n18.7921\n5.47358e-68\n40.785\n50.2956\n\n\ntreatment\n-1.66839\n2.87238\n-0.580839\n0.561476\n-7.30477\n3.96799\n\n\n\nOverall, the analysis results do not support the idea that matching gifts increase donation size once someone is already giving. It is also to be noted that, by restricting the analysis to only people who gave, there is potential conditioning on a post-treatment outcome, and that can introduce selection bias. Therefore, the coefficient no longer reflects a clean causal effect, it’s just a descriptive comparison among people who chose to give. However, in the full sample, since treatment group was randomly assigned, there’s no systematic difference between the treatment and control groups, also verified by the mostly balanced dataset in the analysis in the first section, so the treatment effect can be interpreted as a causal effect.\n\n\nHistogram plots of donation amounts for treatment and control groups, only among people who donated\n\nThe histograms display the distribution of donation amounts among individuals who donated, separated by treatment and control groups. Both groups exhibit a strong right skew, with most donations falling below $100 and a small number of large donations extending the tail of the distribution. The treatment group shows a higher concentration of smaller donations, while the control group has a slightly more even spread, including more mid-sized donations. The mean donation amount is slightly higher in the control group ($45.54) compared to the treatment group ($43.87), but this difference is small and not statistically significant, as seen in the analysis above."
  },
  {
    "objectID": "projects/project 2/index.html#simulation-experiment",
    "href": "projects/project 2/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made (control).\n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made (treatment).\n\n\n\nLaw of Large Numbers\nSimulating 10,000 draws from the control distribution and 10,000 draws from the treatment distribution, using a Bernoulli distribution. I then calculate a vector of 10,000 pairwise differences between each treatment and control outcome, and then plot the cumulative average of that vector of differences.\n\nAt the beginning (small sample sizes), the average is highly variable and “noisy”, sometimes jumping well above or below the true value (0.004).\nAs more simulations are added, the cumulative average should settle down and converges toward the true treatment effect of 0.004, shown by the red dashed line. However, in my plot, the cumulative average converges slightly above the true treatment effect. This is likely due to the random nature of the simulation, as a result of which our sample has slightly more donors than expected in the treatment group/ slightly fewer donors than expected in the control group. Or it could be that in this first run, the simulated sample overestimated the effect of treatment slightly and converged at around 0.007.\n\nUpon running the simulation a second time, and other subsequent runs, a plot similar to the one above is observed, where, as per the Law of Large Numbers the sample average eventually converges to the true average in expectation, at 0.004. This chart shows that even though individual samples may vary due to chance, the average across many independent observations eventually reflects the true underlying effect. This is the statistical foundation behind large-scale experiments and why we rely on big samples for inference.\nThe following Python code simulates 10,000 Bernoulli trials from a control and treatment group, calculates their cumulative average difference, and plots the result.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# True probabilities\np_control = 0.018  # do not get any charitable match\np_treatment = 0.022  # get a charitable match of any size\n\nn = 10000\n\n# Simulate 10,000 Bernoulli draws from each group\ncontrol = np.random.binomial(1, p_control, n)\ntreatment = np.random.binomial(1, p_treatment, n)\n\n# Compute differences between each draw (pairwise)\ndifferences = treatment - control  # each element is 0, +1, or -1\n\n# Cumulative average of the differences\ncumulative_avg_diff = np.cumsum(differences) / np.arange(1, n + 1)\n\n# Plotting\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff, label=\"Cumulative Average of Differences\", color='blue')\nplt.axhline(0.004, color='red', linestyle='--', label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.title(\"Simulation Demonstrating Law of Large Numbers\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nIn order to explore the Central Limit Theorem, I generated four histograms of the average differences between treatment and control groups at sample sizes of 50, 200, 500, and 1000. Each histogram represents the distribution of the average differences in treatment and control groups (using two Bernoulli distributions: control group with a 1.8% donation rate, treatment group with a 2.2% donation rate) from 1000 simulations at each sample size.\n\nThis sequence of histograms demonstrates the Central Limit Theorem by showing how the distribution of average treatment effects becomes more normal and concentrated as the sample size increases. Each histogram represents 1,000 simulated differences in donation rates between treatment and control groups, using sample sizes of 50, 200, 500, and 1,000. At smaller sample sizes, the distribution of average effects is wide and irregular, reflecting high variability. As the sample size grows, the distributions become more bell-shaped, centered near the true treatment effect of 0.004, and show reduced spread. This illustrates that as sample size increases, the sampling distribution of the average treatment effect becomes approximately normal and more precise—just as the Central Limit Theorem predicts.\n\nThe following Python code Simulates 1,000 experiments for each of 4 different sample sizes (n = 50, 200, 500, 1000), collects the 1,000 differences and plots a histogram for each sample size.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Create subplots\nfig, axs = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    \n    for _ in range(num_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(diff)\n    \n    # Plot histogram\n    axs[i].hist(mean_diffs, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(0.004, color='red', linestyle='--', label='True Treatment Effect')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg. Treatment Effect\")\n    axs[i].legend()\n\naxs[0].set_ylabel(\"Frequency\")\nplt.suptitle(\"Demonstrating the Central Limit Theorem\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDetailed Code Jupternotebook\nYou can download and view the full Jupyter notebook that contains the analysis:\n⬇️ Download Notebook"
  },
  {
    "objectID": "projects/project 3/index.html#blueprinty-case-study",
    "href": "projects/project 3/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "Blueprinty Case Study",
    "text": "Blueprinty Case Study\n\nIntroduction\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved. The primary goal is to determine whether firms that use Blueprinty’s software tend to achieve greater patenting success compared to those that do not.\n\n\nData\nThe dataset contains 1,500 rows of data on mature firms as described above. A snippet of it is displayed below along with a brief description of each column.\n\nimport pandas as pd\n\n# Read the CSV file\ndf_bp = pd.read_csv(\"blueprinty.csv\")\n\n# Display the first few rows\ndf_bp.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm in the last 5 years\n\n\nregion\nCategorical region where the firm is located\n\n\nage\nAge of the firm in years since incorporation\n\n\niscustomer\nBinary flag (1 = uses Blueprinty’s software, 0 = does not use)"
  },
  {
    "objectID": "projects/project 3/index.html#data-description-stats",
    "href": "projects/project 3/index.html#data-description-stats",
    "title": "Poisson Regression Examples",
    "section": "Data Description Stats",
    "text": "Data Description Stats\n\nSummary Statistics\n\n\n\nStatistic\nPatents\nAge\nIsCustomer\n\n\n\n\nCount\n1500\n1500\n1500\n\n\nMean\n3.685\n26.358\n0.321\n\n\nStd\n2.352\n7.243\n0.467\n\n\nMin\n0\n9\n0\n\n\n25%\n2\n21\n0\n\n\n50% (Median)\n3\n26\n0\n\n\n75%\n5\n31.625\n1\n\n\nMax\n16\n49\n1\n\n\n\n\nThe summary statistics provide valuable insights into the characteristics of the firms in the dataset. For the ‘patents’ column, firms have an average of 3.7 patents over the past five years, with a standard deviation of 2.35, indicating moderate variability in patent activity. The range spans from 0 to 16 patents, but the 75th percentile is only 5, suggesting that most firms have relatively few patents. The ‘age’ column shows that the firms are generally mature, with an average age of approximately 26.4 years and a maximum age of 49. The interquartile range (21 to 31.6 years) confirms that even the youngest quartile of firms is over two decades old.\nFinally, the ‘iscustomer’ column, which indicates whether a firm uses Blueprinty’s software, has a mean of 0.321, meaning that around 32.1% of the firms are customers. The minimum and maximum values of 0 and 1 confirm the binary nature of this variable. Together, these statistics highlight that the dataset consists of predominantly older firms with modest patent output, and only about a third are currently using the software.\nThe data file is anonymized and does not contain any identifying information for the firms surveyed. It is to be noted that there are 1145 unique entires, and 355 same entries, indicating that there are firms that have the same number of patents, age, and region, or that there may be duplicate entries in the dataset.\n\nHistograms and Means of patents by Customer Status\n\n\n\n\n\n\n\n#Compare histograms and means of number of patents by customer status\n\nimport matplotlib.pyplot as plt\n\n# Clean column names if needed\ndf_bp.columns = df_bp.columns.str.strip().str.lower().str.replace('#', '').str.replace(' ', '_')\n\n# Separate data\ncustomers = df_bp[df_bp['iscustomer'] == 1]\nnon_customers = df_bp[df_bp['iscustomer'] == 0]\n\n# Plot histograms\nplt.figure(figsize=(10, 5))\nplt.hist(non_customers['patents'], bins=20, alpha=0.6, label='Non-Customers')\nplt.hist(customers['patents'], bins=20, alpha=0.6, label='Customers')\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.legend()\nplt.show()\n\n# Compare means\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f\"Mean number of patents (Customers): {mean_customers:.2f}\")\nprint(f\"Mean number of patents (Non-Customers): {mean_non_customers:.2f}\")\n\n\n\n\n\n\n\n\nMean number of patents (Customers): 4.13\nMean number of patents (Non-Customers): 3.47\n\n\n\n\n\nThe data shows that customers of Blueprinty’s software have a higher average number of patents (4.13) compared to non-customers (3.47). While the difference in means, about 0.66 patents, may seem modest, it is notable given the relatively large sample size: 481 customers and 1,019 non-customers. This suggests that firms using Blueprinty’s software may, on average, be more productive or successful in obtaining patents. Although causality cannot be established from this alone, the difference is consistent with the hypothesis that the software may contribute positively to patenting outcomes. Additionally, the customer group represents only about one-third of the sample, yet still shows a higher mean, which may indicate that Blueprinty’s users are either more patent-focused firms or are benefiting from the software in enhancing their patent application success.\nThe histogram indicates that firms using Blueprinty’s software tend to have higher patent counts, pointing to a possible positive association between software usage and patent success. While both customer and non-customer firms are most common in the lower patent range (0-3), non-customers are more concentrated at the lowest counts, whereas customers are more represented in the higher ranges, particularly beyond 6 patents. This suggests that either the software supports greater patent productivity or that more patent-active firms are inclined to use it. However, this observed relationship may be influenced by other variables, such as firm age or regional differences, which warrant further investigation.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nComparing regions and ages by customer status\n\nAge and Customer Status\n\n\n\n\n\n\n\n# Group data by region and customer status\ndf_bp['customer_status'] = df_bp['iscustomer'].map({0: 'Non-Customer', 1: 'Customer'})\nregion_comparison = df_bp.groupby(['region', 'customer_status']).size().unstack()\n\n# Plot grouped bar chart\nregion_comparison.plot(kind='bar', figsize=(10, 6), color=['#DD8452', '#4C72B0'])\nplt.title(\"Region Comparison by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe bar chart shows the regional distribution of firms by customer status. It reveals that the Northeast region has the highest number of customer firms, significantly exceeding the number of non-customers in that area. In contrast, in all other regions—including the Midwest, Northwest, South, and Southwest, non-customers outnumber customers by a wide margin. This suggests that Blueprinty’s customer base is heavily concentrated in the Northeast, which may be a key market for the company. Regional differences like this could influence overall patterns in patenting success and should be considered when interpreting the relationship between software usage and patent outcomes.\n\n\nRegion and Customer Status\n\n\n\n\n\n\n\n# Plot boxplot to compare age by customer status\nimport seaborn as sns\n\nplt.figure(figsize=(10, 5))\nsns.boxplot(data=df_bp, x=\"customer_status\", y=\"age\", palette =['#4C72B0', '#DD8452'])\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status\")\nplt.ylabel(\"Age\")\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_151124/4202040372.py:5: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_bp.groupby(\"iscustomer\")[\"age\"].mean()\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\nThe boxplot compares the age distribution of customer and non-customer firms. It shows that customer firms tend to be slightly older, with a higher median age and a broader upper range compared to non-customers. While both groups have similar minimum ages, the interquartile range (middle 50% of values) is shifted upward for customers, suggesting they are generally more established. Additionally, the distribution for customer firms includes several firms nearing the maximum age of 49, while non-customers show a slightly narrower spread and fewer outliers. The means also show that customers are slightly older on average (26.9 years approx) than non-customers (26.1 years approx), though the difference is fairly small. This indicates that Blueprinty’s software may be more commonly adopted by older, more mature firms, which could influence both their patenting behavior and business needs.\n\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nProbability Mass Function\nGiven that the probability mass function for a single observation Poisson distribution is given by:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!},\n\\]\nwhere \\(Y\\) is the number of patents awarded to a firm in the last 5 years, and \\(\\lambda\\) is the expected number of patents awarded to a firm in the last 5 years. The Poisson distribution is appropriate for modeling count data, particularly when the counts are small integers.\n\n\nLikelihood Function\nThe likelihood function for the Poisson model, assuming that observations are independent, is given by:\n\\[\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^{n} Y_i} \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\n\n\nLog-Likelihood Function\nThe log-likelihood function is given by taking the natural log of the likelihood function. It can be utilized to estimate λ (lambda) through Maximum Likelihood Estimation (MLE). The log-likelihood function is: \\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThe log-likelihood function for the poisson model can be coded as a function of λ and Y as follows:\n\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.special import gammaln \n\n# Likelihood function\ndef poisson_likelihood(lam, Y):\n    if lam &lt;= 0:\n        return 0.0  # Likelihood is zero for non-positive lambda\n    return np.prod((np.exp(-lam) * lam**Y) / np.exp(gammaln(Y + 1)))\n\n# Log-likelihood function (numerically stable)\ndef poisson_log_likelihood(lam, Y):\n    if lam &lt;= 0:\n        return -np.inf  # Log-likelihood is undefined for non-positive lambda\n    return np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n\n\n\n\nThe plots below shows how the likelihood and log-likelihood of the Poisson model varies across different values of λ, using the observed number of patents as input.\n\n\n\n\n\n\n\n# Define the observed number of patents as input for Y\nY = df_bp['patents'].values\n\n# Define a range of lambda values\nlambda_values = np.linspace(0.1, 10, 300)\n\n# Calculate likelihood and log-likelihood for each lambda\nlikelihoods = [poisson_likelihood(lam, Y) for lam in lambda_values]\nlog_likelihoods = [poisson_log_likelihood(lam, Y) for lam in lambda_values]\n\n# Plot the likelihood\nplt.figure(figsize=(12, 6))\nplt.plot(lambda_values, likelihoods, label=\"Likelihood\", color=\"blue\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Likelihood\")\nplt.title(\"Likelihood vs Lambda\")\nplt.grid()\nplt.legend()\nplt.show()\n\n# Plot the log-likelihood\n# Plot the log-likelihood\nplt.figure(figsize=(12, 6))\nplt.plot(lambda_values, log_likelihoods, label=\"Log-Likelihood\", color=\"red\")\n\n# Find the lambda that maximizes the log-likelihood (MLE)\nmle_lambda = lambda_values[np.argmax(log_likelihoods)]\nplt.axvline(mle_lambda, color=\"green\", linestyle=\"--\", label=f\"MLE (Lambda = {mle_lambda:.2f})\")\n\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood vs Lambda\")\nplt.grid()\nplt.legend()\nplt.show()\n\n# Display the MLE\nprint(f\"Maximum Likelihood Estimate (MLE) for Lambda: {mle_lambda:.2f}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimate (MLE) for Lambda: 3.68\n\n\n\n\n\nThe two plots visualize how the Poisson likelihood and log-likelihood functions vary with different values of the parameter λ, based on the observed patent counts. In the first plot, the likelihood values are shown on the vertical axis, but they appear extremely close to zero across the entire range of λ. This is expected when working with products of many small probabilities, as happens when calculating likelihoods over a large dataset. These tiny values are difficult to interpret or visualize directly, which is why plotting the log-likelihood is generally preferred.\nThe second plot displays the log-likelihood as a function of λ, providing a much clearer picture. The curve peaks around λ≈3.68, which corresponds to the maximum likelihood estimate (MLE)—the value of λ that best fits the data under the Poisson model. The log-likelihood function is smooth and concave, indicating a clear maximum, and supports the conclusion that λ=3.68 is the most likely value given the observed data.\nThis estimate reflects the typical patenting rate across the 1,500 firms in your dataset, assuming a Poisson distribution. This aligns closely with the summary statistic (mean patents ≈ 3.69), potentially reinforcing that the model fits the data well. From a business perspective, this could tells us that most mature engineering firms in your sample produce around 3 to 4 patents over a 5-year period, and that this pattern is statistically stable. This average can serve as a benchmark for evaluating Blueprinty’s impact: if their customer firms consistently exceed this rate, and ttheir models confirm it’s not due to other factors (e.g., firm age, region), then there may be a credible argument that Blueprinty’s software supports higher patent success.\n\n\n\nSolving for MLE lambda for the Poisson Model\nThe log-likelihood function for ( n ) independent observations ( Y_1, , Y_n () ) is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ Y_i \\log(\\lambda) - \\lambda - \\log(Y_i!) \\right]\n= \\left( \\sum_{i=1}^n Y_i \\right) \\log(\\lambda) - n\\lambda + \\text{constant}\n\\]\nTo find the maximum likelihood estimate (MLE), take the derivative with respect to ( ), set it equal to zero, and solve:\n\\[\n\\frac{d\\ell}{d\\lambda} = \\frac{\\sum_{i=1}^n Y_i}{\\lambda} - n = 0\n\\]\nSolving for ( ):\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result tells us that the MLE of ( ) is simply the sample mean of the observed data, which makes intuitive sense since the Poisson distribution has mean ( ).\n\n\nMLE estimation using scipy.optimize()\n\n\n\n\n\n\n\n#Find the MLE by optimizing likelihood function with sp.optimize()\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function\ndef negative_log_likelihood(lam, Y):\n    if lam[0] &lt;= 0:\n        return np.inf  # Return infinity for non-positive lambda\n    return -poisson_log_likelihood(lam[0], Y)\n\n# Initial guess for lambda\ninitial_guess = [1.0]\n\n# Perform optimization\nresult = minimize(negative_log_likelihood, initial_guess, args=(Y,), bounds=[(0.001, None)])\n\n# Extract the MLE for lambda\nmle_lambda_optimized = result.x[0]\nprint(f\"Optimized Maximum Likelihood Estimate (MLE) for Lambda: {mle_lambda_optimized:.2f}\")\n\n#Comparing against mean\nmean_patens = df_bp[\"patents\"].mean()\nprint(f\"Mean of Patents: {mean_patens:.2f}\")\n\nOptimized Maximum Likelihood Estimate (MLE) for Lambda: 3.68\nMean of Patents: 3.68\n\n\n\n\n\nThe optimized maximum likelihood estimate (MLE) for λ is 3.68, indicating that the Poisson model estimates the average number of patents per firm in the sample to be approximately 3.68. This value serves as a data-driven benchmark for typical patenting activity among the firms studied. Since the Poisson distribution assumes that the mean and variance are equal, this estimate also reflects the expected variability in patent counts. If firms using Blueprinty’s software are found to have average patent counts significantly above this benchmark, it may suggest a positive association between software usage and patenting success.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nIncluding covariate matrix X in log-likelihood function\n\n\n\n\n\n\n\n# Log-likelihood function with covariates\ndef poisson_log_likelihood_with_covariates(beta, X, Y):\n    \"\"\"\n    Log-likelihood function for Poisson regression with covariates.\n    \n    Parameters:\n    - beta: Coefficient vector (numpy array).\n    - X: Covariate matrix (numpy array).\n    - Y: Observed counts (numpy array).\n    \n    Returns:\n    - Log-likelihood value (float).\n    \"\"\"\n    # Compute lambda using the inverse link function (exp)\n    lambda_ = np.exp(X @ beta)\n    # Compute the log-likelihood\n    return np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\n\n\n\n\n\nFind MLE vector and Hessian of the Poisson model with covariates.\nThis section outlines the steps taken to estimate a Poisson regression model using maximum likelihood estimation. We use scipy.optimize.minimize in Python to obtain the MLE estimates for the coefficient vector β, based on a covariate matrix including age, age squared, regional indicators, and customer status. The Hessian matrix from the optimization is then used to compute standard errors for the coefficient estimates.\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\ndf_bp2 = df_bp.copy()\n\n# Create polynomial term for age\ndf_bp2[\"agesquared\"] = df_bp2[\"age\"] ** 2\n\n# Create dummy variables for region (drop one category to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df_bp2[\"region\"], drop_first=True)\n\n# Construct the design matrix X with intercept, age, agesquared, region dummies, and customer flag\nX = pd.concat([\n    pd.Series(1, index=df_bp2.index, name=\"intercept\"),  # intercept term\n    df_bp2[\"age\"],df_bp2[\"agesquared\"],region_dummies,df_bp2[\"iscustomer\"]], axis=1)\n\n# Response variable\nY = df_bp2[\"patents\"].values\nX_matrix = X.values\n\n# Define Poisson log-likelihood function\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap to avoid overflow in exp\n    lam = np.exp(Xb_clipped)\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\n# Negative log-likelihood for minimization\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n# Initial guess for beta (zeros)\ninitial_beta = np.zeros(X.shape[1])\n\n# Optimize using BFGS\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\n\n# Extract MLE estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\n\n# Create summary table\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\n# Display result\nsummary = summary.round(4)\nsummary\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.5100\n0.1930\n\n\nage\n0.1487\n0.0145\n\n\nagesquared\n-0.0030\n0.0003\n\n\nNortheast\n0.0292\n0.0468\n\n\nNorthwest\n-0.0176\n0.0572\n\n\nSouth\n0.0566\n0.0562\n\n\nSouthwest\n0.0506\n0.0496\n\n\niscustomer\n0.2076\n0.0329\n\n\n\n\n\n\n\n\n\n\nThe Poisson regression results show that firms using Blueprinty’s software have significantly higher expected patent counts, with the iscustomer coefficient of 0.208 indicating about a 23% increase. The positive coefficient for age and negative for agesquared suggest a concave relationship between age and patenting—older firms tend to produce more patents, but at a decreasing rate. Most regional effects are small, with slight increases observed in the South and Southwest. The intercept is negative, reflecting a low baseline patent rate for the reference group. Standard errors are relatively small, indicating precise estimates. Overall, the results support a positive association between software use and patent output.\n\n\nValidation using sm.GLM() function\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Drop the manually added intercept column (to avoid duplication)\nX_sm = X.drop(columns=\"intercept\", errors=\"ignore\")\n\n# Add constant using statsmodels\nX_sm = sm.add_constant(X_sm)\n\n# Ensure all data is numeric\nX_sm = X_sm.astype(float)\n\n# Fit Poisson regression model\npoisson_model = sm.GLM(Y, X_sm, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Show the results\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Thu, 08 May 2025   Deviance:                       2143.3\nTime:                        01:29:23   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage            0.1486      0.014     10.716      0.000       0.121       0.176\nagesquared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\nNortheast      0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest     -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth          0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest      0.0506      0.047      1.072      0.284      -0.042       0.143\niscustomer     0.2076      0.031      6.719      0.000       0.147       0.268\n==============================================================================\n\n\n\n\n\nThe outputs differ slightly because the custom MLE implementation initially included clipping of the linear predictor to prevent numerical overflow, which subtly altered the likelihood surface. In contrast, statsmodels.GLM() uses a more robust internal optimization method without clipping, leading to more precise coefficient estimates.\n\n\n\nInterpretation of Results\nAge has a statistically significant positive effect on patent counts, keeping all else constant, indicating that older firms tend to file more patents. The coefficient for age is 0.1486 (p &lt; 0.001), indicating that, all else equal, each additional year of firm age is associated with approximately a 16% increase in expected patent count (e^0.1486 ≈1.16).\nAgesquared is negative and significant, confirming a diminishing return, holding all else constant, the rate of patenting increases with age but slows down as firms become older. The coefficient for agesquared is -0.0030 (p &lt; 0.001), suggesting diminishing returns to age — the positive effect of age on patenting declines as firms get older.\nBlueprinty’s software use is associated with a significant increase in patent output.The coefficient for iscustomer is 0.2076 (p &lt; 0.001), implying that firms using Blueprinty’s software are expected to have about 23% more patents than non-customers (e^0.2076 ≈ 1.23), holding other factors constant. The intercept is -0.5089, representing the expected log count of patents for the base firm: a non-customer located in the excluded region, with age and agesquared equal to zero (not interpretable directly but useful for model fit).\nRegional effects are small and not statistically significant, suggesting that geographic location does not have a meaningful impact on patent activity after accounting for age and software usage, keeping all else constant. For example, the Northeast coefficient is 0.0292 (p = 0.504), Northwest is -0.0176 (p = 0.744), South is 0.0566 (p = 0.283), and Southwest is 0.0506 (p = 0.284), indicating no meaningful difference in patenting by region. The model log-likelihood is -3258.1, and the Pseudo R-squared (Cragg-Uhler/CS) is 0.1360, suggesting a moderate model fit.\n\n\nConclusion about the effect of Blueprinty’s software on patent success\nBecause the beta coefficients are not directly interpretable, the following approach is sued: 1. Creating two scenarios of datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every customer observation and X_1 is the X data but with iscustomer=1 for every customer observation.\n2. Comparing the predicted number of patents for each firm utilizing X_0 and the fitted model by (i)getting the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and using X_1 to get Y_pred_1 for every firm, and (ii)subtracting y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n\n\n\n\n\n\n\n# Copy the original design matrix used in GLM\nX_0 = X_sm.copy()\nX_1 = X_sm.copy()\n\n# Set all iscustomer values to 0 and 1 respectively\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patents using the fitted model\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Compute the average difference in expected patent counts\naverage_effect = (y_pred_1 - y_pred_0).mean()\nprint(f\"Estimated average effect of Blueprinty's software: {average_effect:.4f} patents per firm\")\n\nEstimated average effect of Blueprinty's software: 0.7928 patents per firm\n\n\nOn average, firms using Blueprinty’s software are expected to have 0.793 more patents than they would have had if they were not using the software, over five years, controlling for other factors like age and region.\nThis result provides a clear and practical interpretation of the model: the use of Blueprinty’s software is associated with a substantial increase in patenting activity. Given the sample mean of patents is around 3.68, an increase of 0.79 patents represents a 21.5% improvement, which reinforces the idea that Blueprinty’s product may meaningfully enhance patent success for its customers.\n\nAirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The number of reviews on a listing is used as a proxy for booking activity, under the assumption that more bookings typically lead to more reviews. The goal is to understand which listing characteristics—such as room type, price, review scores, and instant bookability—are associated with higher engagement from guests. The analysis begins with exploratory data visualization to assess variable distributions and correlations, followed by a Poisson regression model, which is well-suited for count data like reviews.\nThe data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nExploratory Data Analysis\n\nData Cleaning\nThe dataset contains 40,628 rows and 14 columns. The first step is to clean the data by removing any unnecessary columns and handling missing values. The id and unnanmed:0 column are not needed for analysis, so they will be dropped. The last_scraped and host_since columns are also not needed for the analysis, as they do not provide useful information for modeling the number of reviews.\n\n\n\n\n\n\n\nimport pandas as pd\ndf_ab = pd.read_csv('airbnb.csv')\n\n#Drop unnecessary columns and missing values \ncols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf = df_ab[cols].dropna()\n\n\n\n\n\n\nData Visualization\nHistograms for numerical features\n\n\n\n\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Histograms for numerical features\nnumeric_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms',\n                'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\ndf[numeric_cols].hist(bins=30, figsize=(14, 10))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe histograms of the numerical features reveal the following:\n\nNumber of Reviews: Highly right-skewed. Most listings have relatively few reviews, with a small number having over 100, indicating a few very popular listings dominate total review counts.\n\nPrice: Also heavily right-skewed. The vast majority of listings are priced under $500 per night, with a few extreme outliers reaching up to $10,000.\n\nBathrooms & Bedrooms: Most listings have 1 bathroom and 1 bedroom, which is expected for urban short-term rentals. Larger units are rare.\n\nReview Scores (Cleanliness, Location, Value): These are all left-skewed, meaning most listings receive high scores (especially 9s and 10s), which is common in user-review platforms due to rating inflation or selection bias.\n\nOverall, the distributions suggest that the dataset is dominated by reasonably priced, small units with high review ratings and a wide spread in listing popularity (as measured by review count).\nCorrelation Heatmap between Numerical Features\n\n\n\n\n\n\n\n# Correlation heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe correlation heatmap shows that the number of reviews, used as a proxy for bookings, has very weak correlations with all other numeric variables. It is essentially uncorrelated with price, size (bathrooms and bedrooms), and review scores. This suggests that popularity is not directly tied to these features alone, and other unmeasured factors such as location specifics, host responsiveness, or listing visibility may be influencing review volume.\nIn contrast, there are moderate positive correlations among the review score variables. For instance, cleanliness and value (r = 0.62), and cleanliness and location (r = 0.33), are moderately correlated, suggesting that guests who rate one aspect of a stay highly are likely to rate others highly as well. Bedrooms and bathrooms are also fairly correlated (r = 0.41), which is intuitive since larger units tend to have more of both.\nOverall, while listing features and review scores tend to co-vary internally, they do not individually explain much of the variation in how frequently listings are reviewed.\nDistribution of Number of Reviews\n\n\n\n\n\n\n\n# Plot distribution of number_of_reviews\nplt.figure(figsize=(8, 5))\nsns.histplot(df['number_of_reviews'], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of the number of reviews is highly right-skewed, indicating that while a small number of listings receive a large volume of reviews, the vast majority receive relatively few. Most listings have fewer than 20 reviews, with a sharp drop-off beyond that point. Only a small number of listings exceed 100 reviews, and very few surpass 200. This suggests that a limited set of listings likely dominate guest attention and bookings, possibly due to factors like high visibility, competitive pricing, or outstanding host performance. The skewed nature of the distribution supports the use of a Poisson or count-based model for analyzing the data.\nNumber of Reviews by Room Type\n\n\n\n\n\n\n\n# Average number of reviews by room_type\nplt.figure(figsize=(8, 5))\nsns.barplot(x='room_type', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Room Type\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n# Boxplot of number_of_reviews by room_type\nsns.boxplot(x='room_type', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Room Type')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two plots together provide insight into how room type relates to the number of reviews on Airbnb, which we treat as a proxy for bookings. The bar plot shows that private rooms and entire homes/apartments have very similar average review counts, just over 21 reviews, while shared rooms lag behind with a noticeably lower average of around 17 reviews. This suggests that private and entire-unit listings are more frequently booked than shared rooms, on average.\nThe boxplot complements this by showing the distribution and spread of reviews. While all room types have many low-review listings and some high outliers, private rooms appear to have a slightly broader spread of review counts and more extreme outliers, indicating that they are more variable in popularity. Shared rooms, by contrast, have both a lower median and fewer high-review outliers. Together, these visuals suggest that private rooms and entire units are generally more popular among guests, with private rooms possibly attracting a wider range of booking frequencies.\nNumber of Reviews by Instant Bookability\n\n\n\n\n\n\n\n# Average number of reviews by instant_bookable\nplt.figure(figsize=(8, 5))\nsns.barplot(x='instant_bookable', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Instant Bookable Status\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n# Boxplot of number_of_reviews by instant_bookable\nsns.boxplot(x='instant_bookable', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Instant Bookable Status')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two plots illustrate the relationship between instant bookability and the number of reviews on Airbnb listings. The bar plot shows that listings marked as instantly bookable (“t”) receive, on average, about 8 more reviews than those that are not (“f”) roughly 28 reviews vs. 20. This suggests a clear difference in average popularity or booking frequency between the two groups. The boxplot reinforces this finding: the distribution of reviews for instant-bookable listings is slightly higher across the board, with a higher median and a greater concentration of listings in the upper range of reviews. Both groups exhibit heavy right-skewness and contain high-outlier listings, but the instantly bookable group tends to outperform overall. Together, the plots suggest that instant bookability is associated with more reviews, which supports the idea that offering convenience and flexibility may drive more bookings.\nNumber of Reviews by Number of Bedrooms\n\n\n\n\n\n\n\n# Average number of reviews by room_type and instant_bookable (interaction)\nplt.figure(figsize=(8, 5))\nsns.barplot(x='bedrooms', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Number of Bedrooms\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n#Boxplot of number_of_reviews by bedrooms\nsns.boxplot(x='bedrooms', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Bedrooms')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two plots examine how the number of bedrooms in an Airbnb listing relates to the number of reviews (i.e., bookings). The boxplot shows that listings with 0 to 2 bedrooms are by far the most common and tend to have the highest concentration of reviews. Listings with 1 bedroom, in particular, have the widest spread and many high-outlier values, suggesting they are the most frequently booked. As the number of bedrooms increases, the number of reviews becomes more sparse, with a generally lower median and fewer high-review outliers.\nThe bar plot reinforces this trend: listings with 1–3 bedrooms have relatively high average review counts (around 22–25), while listings with 6 or more bedrooms see more variability and tend to have lower average reviews, though with large error bars due to smaller sample sizes. Notably, the average reviews for 8-bedroom listings spike, but the wide error bars suggest this is based on very few listings and should be interpreted cautiously.Together, these plots suggest that smaller listings, particularly 1-bedroom units, are more popular and frequently booked, likely because they cater to solo travelers or couples, who represent a large portion of Airbnb users. Larger listings may cater to niche or group travel markets and receive fewer bookings overall.\nAverage number of reviews by room_type and instant_bookable\n\n\n\n\n\n\n\n# Average number of reviews by room_type and instant_bookable (interaction)\nplt.figure(figsize=(10, 6))\nsns.barplot(x='room_type', y='number_of_reviews', hue='instant_bookable', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Room Type and Instant Bookability\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.legend(title='Instant Bookable')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis plot shows the average number of reviews by room type, broken down by instant bookability. It reveals a consistent pattern across all room types: listings that are instantly bookable receive more reviews on average than those that are not. For both private rooms and entire homes/apartments, instant-bookable listings average around 27–29 reviews, compared to only 19–20 reviews for non-instant listings.Even for shared rooms, which have the lowest review counts overall, enabling instant booking still leads to a modest increase in average reviews.\nThis suggests that instant bookability enhances booking volume regardless of room type, with the most pronounced benefit observed in private rooms and entire units. It highlights the potential value of enabling instant booking as a strategy to boost listing popularity and visibility.\n\n\n\nPoisson Regression Model\nTo better understand which listing characteristics are associated with higher booking activity, we fit a Poisson regression model using the number of reviews as a proxy for the number of bookings. This model is appropriate for count data and allows us to examine how various features including price, number of bedrooms and bathrooms, review scores, room type, and instant bookability, contribute to variation in review counts. By converting categorical variables to dummy indicators and standardizing the design matrix, we estimate the effect of each factor while controlling for the others. We then identify which predictors have a statistically significant association with review volume.\n\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Create dummy variables for room type and mao instant bookable to binary\nroom_dummies = pd.get_dummies(df[\"room_type\"], drop_first=True)\ndf[\"instant_bookable\"] = df[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Construct the design matrix\nX = pd.concat([df[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\"instant_bookable\"]],room_dummies], axis=1)\n\n# Add intercept and ensure all values are float\nX = sm.add_constant(X)\nX = X.astype(float)\n\n# Define the target variable\nY = df[\"number_of_reviews\"]\n\n# Fit Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Format output\nsummary_df = poisson_results.summary2().tables[1]\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P&gt;|z|\": \"P-Value\"\n})\n\nprint(summary_df.round(4))\n\n                           Coefficient  Std. Error         z  P-Value  [0.025  \\\nconst                           3.4980      0.0161  217.3963   0.0000  3.4665   \nprice                          -0.0000      0.0000   -2.1509   0.0315 -0.0000   \ndays                            0.0001      0.0000  129.7553   0.0000  0.0000   \nbathrooms                      -0.1177      0.0037  -31.3942   0.0000 -0.1251   \nbedrooms                        0.0741      0.0020   37.1972   0.0000  0.0702   \nreview_scores_cleanliness       0.1131      0.0015   75.6106   0.0000  0.1102   \nreview_scores_location         -0.0769      0.0016  -47.7962   0.0000 -0.0801   \nreview_scores_value            -0.0911      0.0018  -50.4899   0.0000 -0.0946   \ninstant_bookable                0.3459      0.0029  119.6656   0.0000  0.3402   \nPrivate room                   -0.0105      0.0027   -3.8475   0.0001 -0.0159   \nShared room                    -0.2463      0.0086  -28.5781   0.0000 -0.2632   \n\n                           0.975]  \nconst                      3.5296  \nprice                     -0.0000  \ndays                       0.0001  \nbathrooms                 -0.1104  \nbedrooms                   0.0780  \nreview_scores_cleanliness  0.1161  \nreview_scores_location    -0.0737  \nreview_scores_value       -0.0875  \ninstant_bookable           0.3515  \nPrivate room              -0.0052  \nShared room               -0.2294  \n\n\n\n\n\nShowing only significant results:\n\n\n\n\n\n\n\n# Filter and round significant results\nsummary_results = summary_df[summary_df[\"P-Value\"] &lt; 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\nsummary_results = summary_results.round(4)\nprint(summary_results)\n\n                           Coefficient  Std. Error  P-Value\nconst                           3.4980      0.0161   0.0000\nprice                          -0.0000      0.0000   0.0315\ndays                            0.0001      0.0000   0.0000\nbathrooms                      -0.1177      0.0037   0.0000\nbedrooms                        0.0741      0.0020   0.0000\nreview_scores_cleanliness       0.1131      0.0015   0.0000\nreview_scores_location         -0.0769      0.0016   0.0000\nreview_scores_value            -0.0911      0.0018   0.0000\ninstant_bookable                0.3459      0.0029   0.0000\nPrivate room                   -0.0105      0.0027   0.0001\nShared room                    -0.2463      0.0086   0.0000\n\n\n\n\n\nThe Poisson regression model reveals several statistically significant predictors of the number of reviews, used here as a proxy for booking activity. The intercept represents the baseline log-expected number of reviews for a listing when all other variables are set to zero. This would correspond to a listing with zero price, zero days on the platform, no bathrooms or bedrooms, no review scores, not instantly bookable, and listed as the reference room type (entire home/apartment). Although this profile is not realistic, the intercept provides a baseline from which all other effects are interpreted.\nThe coefficient for instant bookable is 0.3459 (p &lt; 0.001), implying that listings that allow instant booking receive approximately 41% more reviews than those that do not, holding all other variables constant (e^0.3459 ≈ 1.41).Review scores for cleanliness are positively associated with reviews: a one-point increase in cleanliness rating is linked to a 12% increase in expected reviews (e^0.1131 ≈1.12).\nIn contrast, higher scores for location and value are surprisingly associated with fewer reviews, though the effect sizes are modest (-7.7% and -8.7%,respectively), holding all other variables constant, and the strong statistical significance suggests consistent patterns.\nListings with more bedrooms tend to receive more reviews, with each additional bedroom associated with an 8% increase in review count (e^0.0741 ≈1.08), while listings with more bathrooms receive fewer reviews, holding all other variables constant. The age of the listing (days on the platform) also has a positive but very small effect per unit, reflecting the cumulative nature of reviews over time.\nIn terms of room type, shared rooms receive significantly fewer reviews than entire homes, with a 22% lower review count, while private rooms are only slightly less reviewed than entire homes, keeping all else constant. Lastly, price has a very small but negative effect on review volume, indicating that higher-priced listings may attract slightly fewer bookings, keeping all else constant.\nOverall, the results suggest that instant bookability, higher cleanliness ratings, and more bedrooms are the strongest positive predictors of listing popularity, while shared rooms and higher prices are associated with lower engagement."
  },
  {
    "objectID": "projects/project 3/index.html#airbnb-case-study",
    "href": "projects/project 3/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The number of reviews on a listing is used as a proxy for booking activity, under the assumption that more bookings typically lead to more reviews. The goal is to understand which listing characteristics—such as room type, price, review scores, and instant bookability—are associated with higher engagement from guests. The analysis begins with exploratory data visualization to assess variable distributions and correlations, followed by a Poisson regression model, which is well-suited for count data like reviews.\nThe data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nExploratory Data Analysis\n\nData Cleaning\nThe dataset contains 40,628 rows and 14 columns. The first step is to clean the data by removing any unnecessary columns and handling missing values. The id and unnanmed:0 column are not needed for analysis, so they will be dropped. The last_scraped and host_since columns are also not needed for the analysis, as they do not provide useful information for modeling the number of reviews.\n\n\n\n\n\n\n\nimport pandas as pd\ndf_ab = pd.read_csv('airbnb.csv')\n\n#Drop unnecessary columns and missing values \ncols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf = df_ab[cols].dropna()\n\n\n\n\n\n\nData Visualization\nHistograms for numerical features\n\n\n\n\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Histograms for numerical features\nnumeric_cols = ['number_of_reviews', 'price', 'bathrooms', 'bedrooms',\n                'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\ndf[numeric_cols].hist(bins=30, figsize=(14, 10))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe histograms of the numerical features reveal the following:\n\nNumber of Reviews: Highly right-skewed. Most listings have relatively few reviews, with a small number having over 100, indicating a few very popular listings dominate total review counts.\n\nPrice: Also heavily right-skewed. The vast majority of listings are priced under $500 per night, with a few extreme outliers reaching up to $10,000.\n\nBathrooms & Bedrooms: Most listings have 1 bathroom and 1 bedroom, which is expected for urban short-term rentals. Larger units are rare.\n\nReview Scores (Cleanliness, Location, Value): These are all left-skewed, meaning most listings receive high scores (especially 9s and 10s), which is common in user-review platforms due to rating inflation or selection bias.\n\nOverall, the distributions suggest that the dataset is dominated by reasonably priced, small units with high review ratings and a wide spread in listing popularity (as measured by review count).\nCorrelation Heatmap between Numerical Features\n\n\n\n\n\n\n\n# Correlation heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe correlation heatmap shows that the number of reviews, used as a proxy for bookings, has very weak correlations with all other numeric variables. It is essentially uncorrelated with price, size (bathrooms and bedrooms), and review scores. This suggests that popularity is not directly tied to these features alone, and other unmeasured factors such as location specifics, host responsiveness, or listing visibility may be influencing review volume.\nIn contrast, there are moderate positive correlations among the review score variables. For instance, cleanliness and value (r = 0.62), and cleanliness and location (r = 0.33), are moderately correlated, suggesting that guests who rate one aspect of a stay highly are likely to rate others highly as well. Bedrooms and bathrooms are also fairly correlated (r = 0.41), which is intuitive since larger units tend to have more of both.\nOverall, while listing features and review scores tend to co-vary internally, they do not individually explain much of the variation in how frequently listings are reviewed.\nDistribution of Number of Reviews\n\n\n\n\n\n\n\n# Plot distribution of number_of_reviews\nplt.figure(figsize=(8, 5))\nsns.histplot(df['number_of_reviews'], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of the number of reviews is highly right-skewed, indicating that while a small number of listings receive a large volume of reviews, the vast majority receive relatively few. Most listings have fewer than 20 reviews, with a sharp drop-off beyond that point. Only a small number of listings exceed 100 reviews, and very few surpass 200. This suggests that a limited set of listings likely dominate guest attention and bookings, possibly due to factors like high visibility, competitive pricing, or outstanding host performance. The skewed nature of the distribution supports the use of a Poisson or count-based model for analyzing the data.\nNumber of Reviews by Room Type\n\n\n\n\n\n\n\n# Average number of reviews by room_type\nplt.figure(figsize=(8, 5))\nsns.barplot(x='room_type', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Room Type\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n# Boxplot of number_of_reviews by room_type\nsns.boxplot(x='room_type', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Room Type')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two plots together provide insight into how room type relates to the number of reviews on Airbnb, which we treat as a proxy for bookings. The bar plot shows that private rooms and entire homes/apartments have very similar average review counts, just over 21 reviews, while shared rooms lag behind with a noticeably lower average of around 17 reviews. This suggests that private and entire-unit listings are more frequently booked than shared rooms, on average.\nThe boxplot complements this by showing the distribution and spread of reviews. While all room types have many low-review listings and some high outliers, private rooms appear to have a slightly broader spread of review counts and more extreme outliers, indicating that they are more variable in popularity. Shared rooms, by contrast, have both a lower median and fewer high-review outliers. Together, these visuals suggest that private rooms and entire units are generally more popular among guests, with private rooms possibly attracting a wider range of booking frequencies.\nNumber of Reviews by Instant Bookability\n\n\n\n\n\n\n\n# Average number of reviews by instant_bookable\nplt.figure(figsize=(8, 5))\nsns.barplot(x='instant_bookable', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Instant Bookable Status\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n# Boxplot of number_of_reviews by instant_bookable\nsns.boxplot(x='instant_bookable', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Instant Bookable Status')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two plots illustrate the relationship between instant bookability and the number of reviews on Airbnb listings. The bar plot shows that listings marked as instantly bookable (“t”) receive, on average, about 8 more reviews than those that are not (“f”) roughly 28 reviews vs. 20. This suggests a clear difference in average popularity or booking frequency between the two groups. The boxplot reinforces this finding: the distribution of reviews for instant-bookable listings is slightly higher across the board, with a higher median and a greater concentration of listings in the upper range of reviews. Both groups exhibit heavy right-skewness and contain high-outlier listings, but the instantly bookable group tends to outperform overall. Together, the plots suggest that instant bookability is associated with more reviews, which supports the idea that offering convenience and flexibility may drive more bookings.\nNumber of Reviews by Number of Bedrooms\n\n\n\n\n\n\n\n# Average number of reviews by room_type and instant_bookable (interaction)\nplt.figure(figsize=(8, 5))\nsns.barplot(x='bedrooms', y='number_of_reviews', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Number of Bedrooms\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n#Boxplot of number_of_reviews by bedrooms\nsns.boxplot(x='bedrooms', y='number_of_reviews', data=df)\nplt.title('Number of Reviews by Bedrooms')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two plots examine how the number of bedrooms in an Airbnb listing relates to the number of reviews (i.e., bookings). The boxplot shows that listings with 0 to 2 bedrooms are by far the most common and tend to have the highest concentration of reviews. Listings with 1 bedroom, in particular, have the widest spread and many high-outlier values, suggesting they are the most frequently booked. As the number of bedrooms increases, the number of reviews becomes more sparse, with a generally lower median and fewer high-review outliers.\nThe bar plot reinforces this trend: listings with 1–3 bedrooms have relatively high average review counts (around 22–25), while listings with 6 or more bedrooms see more variability and tend to have lower average reviews, though with large error bars due to smaller sample sizes. Notably, the average reviews for 8-bedroom listings spike, but the wide error bars suggest this is based on very few listings and should be interpreted cautiously.Together, these plots suggest that smaller listings, particularly 1-bedroom units, are more popular and frequently booked, likely because they cater to solo travelers or couples, who represent a large portion of Airbnb users. Larger listings may cater to niche or group travel markets and receive fewer bookings overall.\nAverage number of reviews by room_type and instant_bookable\n\n\n\n\n\n\n\n# Average number of reviews by room_type and instant_bookable (interaction)\nplt.figure(figsize=(10, 6))\nsns.barplot(x='room_type', y='number_of_reviews', hue='instant_bookable', data=df, estimator=np.mean)\nplt.title(\"Average Number of Reviews by Room Type and Instant Bookability\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.legend(title='Instant Bookable')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThis plot shows the average number of reviews by room type, broken down by instant bookability. It reveals a consistent pattern across all room types: listings that are instantly bookable receive more reviews on average than those that are not. For both private rooms and entire homes/apartments, instant-bookable listings average around 27–29 reviews, compared to only 19–20 reviews for non-instant listings.Even for shared rooms, which have the lowest review counts overall, enabling instant booking still leads to a modest increase in average reviews.\nThis suggests that instant bookability enhances booking volume regardless of room type, with the most pronounced benefit observed in private rooms and entire units. It highlights the potential value of enabling instant booking as a strategy to boost listing popularity and visibility.\n\n\n\nPoisson Regression Model\nTo better understand which listing characteristics are associated with higher booking activity, we fit a Poisson regression model using the number of reviews as a proxy for the number of bookings. This model is appropriate for count data and allows us to examine how various features including price, number of bedrooms and bathrooms, review scores, room type, and instant bookability, contribute to variation in review counts. By converting categorical variables to dummy indicators and standardizing the design matrix, we estimate the effect of each factor while controlling for the others. We then identify which predictors have a statistically significant association with review volume.\n\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Create dummy variables for room type and mao instant bookable to binary\nroom_dummies = pd.get_dummies(df[\"room_type\"], drop_first=True)\ndf[\"instant_bookable\"] = df[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Construct the design matrix\nX = pd.concat([df[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\"instant_bookable\"]],room_dummies], axis=1)\n\n# Add intercept and ensure all values are float\nX = sm.add_constant(X)\nX = X.astype(float)\n\n# Define the target variable\nY = df[\"number_of_reviews\"]\n\n# Fit Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Format output\nsummary_df = poisson_results.summary2().tables[1]\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P&gt;|z|\": \"P-Value\"\n})\n\nprint(summary_df.round(4))\n\n                           Coefficient  Std. Error         z  P-Value  [0.025  \\\nconst                           3.4980      0.0161  217.3963   0.0000  3.4665   \nprice                          -0.0000      0.0000   -2.1509   0.0315 -0.0000   \ndays                            0.0001      0.0000  129.7553   0.0000  0.0000   \nbathrooms                      -0.1177      0.0037  -31.3942   0.0000 -0.1251   \nbedrooms                        0.0741      0.0020   37.1972   0.0000  0.0702   \nreview_scores_cleanliness       0.1131      0.0015   75.6106   0.0000  0.1102   \nreview_scores_location         -0.0769      0.0016  -47.7962   0.0000 -0.0801   \nreview_scores_value            -0.0911      0.0018  -50.4899   0.0000 -0.0946   \ninstant_bookable                0.3459      0.0029  119.6656   0.0000  0.3402   \nPrivate room                   -0.0105      0.0027   -3.8475   0.0001 -0.0159   \nShared room                    -0.2463      0.0086  -28.5781   0.0000 -0.2632   \n\n                           0.975]  \nconst                      3.5296  \nprice                     -0.0000  \ndays                       0.0001  \nbathrooms                 -0.1104  \nbedrooms                   0.0780  \nreview_scores_cleanliness  0.1161  \nreview_scores_location    -0.0737  \nreview_scores_value       -0.0875  \ninstant_bookable           0.3515  \nPrivate room              -0.0052  \nShared room               -0.2294  \n\n\n\n\n\nShowing only significant results:\n\n\n\n\n\n\n\n# Filter and round significant results\nsummary_results = summary_df[summary_df[\"P-Value\"] &lt; 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\nsummary_results = summary_results.round(4)\nprint(summary_results)\n\n                           Coefficient  Std. Error  P-Value\nconst                           3.4980      0.0161   0.0000\nprice                          -0.0000      0.0000   0.0315\ndays                            0.0001      0.0000   0.0000\nbathrooms                      -0.1177      0.0037   0.0000\nbedrooms                        0.0741      0.0020   0.0000\nreview_scores_cleanliness       0.1131      0.0015   0.0000\nreview_scores_location         -0.0769      0.0016   0.0000\nreview_scores_value            -0.0911      0.0018   0.0000\ninstant_bookable                0.3459      0.0029   0.0000\nPrivate room                   -0.0105      0.0027   0.0001\nShared room                    -0.2463      0.0086   0.0000\n\n\n\n\n\nThe Poisson regression model reveals several statistically significant predictors of the number of reviews, used here as a proxy for booking activity. The intercept represents the baseline log-expected number of reviews for a listing when all other variables are set to zero. This would correspond to a listing with zero price, zero days on the platform, no bathrooms or bedrooms, no review scores, not instantly bookable, and listed as the reference room type (entire home/apartment). Although this profile is not realistic, the intercept provides a baseline from which all other effects are interpreted.\nThe coefficient for instant bookable is 0.3459 (p &lt; 0.001), implying that listings that allow instant booking receive approximately 41% more reviews than those that do not, holding all other variables constant (e^0.3459 ≈ 1.41).Review scores for cleanliness are positively associated with reviews: a one-point increase in cleanliness rating is linked to a 12% increase in expected reviews (e^0.1131 ≈1.12).\nIn contrast, higher scores for location and value are surprisingly associated with fewer reviews, though the effect sizes are modest (-7.7% and -8.7%,respectively), holding all other variables constant, and the strong statistical significance suggests consistent patterns.\nListings with more bedrooms tend to receive more reviews, with each additional bedroom associated with an 8% increase in review count (e^0.0741 ≈1.08), while listings with more bathrooms receive fewer reviews, holding all other variables constant. The age of the listing (days on the platform) also has a positive but very small effect per unit, reflecting the cumulative nature of reviews over time.\nIn terms of room type, shared rooms receive significantly fewer reviews than entire homes, with a 22% lower review count, while private rooms are only slightly less reviewed than entire homes, keeping all else constant. Lastly, price has a very small but negative effect on review volume, indicating that higher-priced listings may attract slightly fewer bookings, keeping all else constant.\nOverall, the results suggest that instant bookability, higher cleanliness ratings, and more bedrooms are the strongest positive predictors of listing popularity, while shared rooms and higher prices are associated with lower engagement."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\nNitya Kowsalya Vootla\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nNitya Kowsalya Vootla\nMay 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Late Delivery Risk for a Sport Retail Business\n\n\n\n\nNitya Kowsalya Vootla\nDec 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]